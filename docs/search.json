[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TAZ & Regional Median Income",
    "section": "",
    "text": "This analysis calculates median household income for Transportation Analysis Zones (TAZ) across Utah using data from the 2019-2023 American Community Survey (ACS). The methodology employs spatial interpolation techniques to transfer income data from Census geographies (block groups and blocks) to TAZ boundaries, accounting for spatial overlaps and using household counts as weights.\nThe process involves several key steps: obtaining income data at the block group level, distributing this data to census blocks, and then aggregating to TAZ boundaries using area-weighted interpolation. This ensures that income estimates accurately reflect the spatial distribution of households within each TAZ."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "TAZ & Regional Median Income",
    "section": "",
    "text": "This analysis calculates median household income for Transportation Analysis Zones (TAZ) across Utah using data from the 2019-2023 American Community Survey (ACS). The methodology employs spatial interpolation techniques to transfer income data from Census geographies (block groups and blocks) to TAZ boundaries, accounting for spatial overlaps and using household counts as weights.\nThe process involves several key steps: obtaining income data at the block group level, distributing this data to census blocks, and then aggregating to TAZ boundaries using area-weighted interpolation. This ensures that income estimates accurately reflect the spatial distribution of households within each TAZ."
  },
  {
    "objectID": "index.html#environment-setup",
    "href": "index.html#environment-setup",
    "title": "TAZ & Regional Median Income",
    "section": "2 Environment Setup",
    "text": "2 Environment Setup\n\nInstall Required Packages\nInstall the necessary Python packages for data processing, spatial analysis, and visualization. This includes pandas for data manipulation, geopandas for spatial operations, pygris for accessing Census TIGER/Line shapefiles, and various visualization libraries.\n!conda install -c conda-forge numpy pandas geopandas matplotlib seaborn python-dotenv openpyxl\n!pip install pygris adjustText\n\n\nLoad Libraries\nImport all required libraries for the analysis. The pygris library enables direct access to Census Bureau geographic data and ACS estimates through their API.\n\n\nShow the code\n# For Analysis\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport warnings\n\n# For Visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nfrom adjustText import adjust_text\n\n# Census data query libraries & modules\nfrom pygris import blocks, block_groups, counties, states\nfrom pygris.helpers import validate_state, validate_county\nfrom pygris.data import get_census\n\n# misc\nimport datetime\nimport os\nfrom pathlib import Path\nimport requests\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\n\nTrue\n\n\n\n\nEnvironment Variables\nSet the coordinate reference system for the analysis to NAD83 / UTM zone 12N (EPSG:3566), which is appropriate for Utah. Also validate the state FIPS code for Utah.\n\n\nShow the code\nPROJECT_CRS = \"EPSG:3566\"  # NAD83 / UTM zone 12N\nSTATE_FIPS = validate_state(\"UT\")\n\n\nUsing FIPS code '49' for input 'UT'\n\n\n\n\n\n\n\n\nTip\n\n\n\nNeed a Census API key? Get one for free at census.gov/developers.\nCreate a .env file in the project directory and add your Census API key: CENSUS_API_KEY=your-key-here This enables fetching US Census data from the Census API.\n\n\n\n\nShow the code\n# Set your API key into environment (alternative to .env file)\nos.environ['CENSUS_API_KEY'] = 'your_api_key_here'"
  },
  {
    "objectID": "index.html#define-helper-functions",
    "href": "index.html#define-helper-functions",
    "title": "TAZ & Regional Median Income",
    "section": "3 Define Helper Functions",
    "text": "3 Define Helper Functions\nTo maintain code reusability and follow DRY (Don’t Repeat Yourself) principles, we define helper functions for common operations throughout the analysis.\n\nFetch Excel Files from BLS or BTS\nThis utility function automates downloading data files from federal agencies. It checks if a file already exists locally before attempting to download, avoiding unnecessary network requests and respecting the agencies’ servers. The function includes proper HTTP headers to ensure reliable downloads.\n\n\nShow the code\ndef fetch_excel(path, url):\n    \"\"\"\n    Download Excel file if it doesn't exist locally.\n\n    Parameters:\n    -----------\n    path : str or Path\n        Local file path to save the Excel file\n    url : str\n        URL to download the Excel file from\n    \"\"\"\n    # Convert to Path object if string\n    filepath = Path(path)\n\n    # Download file if it doesn't exist\n    if not filepath.exists():\n        filepath.parent.mkdir(parents=True, exist_ok=True)\n\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        }\n\n        response = requests.get(url, headers=headers)\n        filepath.write_bytes(response.content)\n\n\n\n\nWeighted Spatial Interpolation Function\nThis custom function performs population-weighted areal interpolation, which is a sophisticated method for transferring data between different geographic units. Traditional spatial joins often use centroids or simple overlaps, which can introduce errors when dealing with irregularly shaped polygons or when population is not evenly distributed.\nThe interpolate_pw function addresses these limitations by:\n\nUsing point-based weights (like household counts) to represent where population actually lives within each geography\nCalculating how much of each weight falls within intersected areas\nProportionally distributing values based on these weights rather than just area\n\nThis approach is particularly important for income data, where we want to account for where households are located, not just land area.\n\n\nShow the code\ndef interpolate_pw(from_gdf, to_gdf, weights_gdf, to_id=None, extensive=True,\n                   weight_column=None, weight_placement='surface', crs=None):\n    \"\"\"\n    Population-weighted areal interpolation between geometries.\n\n    Transfers numeric data from source geometries to target geometries using\n    population-weighted interpolation based on point weights (e.g., census blocks).\n\n    Parameters:\n    -----------\n    from_gdf : GeoDataFrame\n        Source geometries with numeric data to interpolate\n    to_gdf : GeoDataFrame\n        Target geometries to interpolate data to\n    weights_gdf : GeoDataFrame\n        Weight geometries (e.g., census blocks) used for interpolation.\n        If polygons, will be converted to points. Can be the same as to_gdf.\n    to_id : str, optional\n        Column name for unique identifier in target geometries.\n        If None, creates an 'id' column.\n    extensive : bool, default True\n        If True, return weighted sums (for counts).\n        If False, return weighted means (for rates/percentages).\n    weight_column : str, optional\n        Column name in weights_gdf for weighting (e.g., 'POP', 'HH').\n        If None, all weights are equal.\n    weight_placement : str, default 'surface'\n        How to convert polygons to points: 'surface' or 'centroid'\n    crs : str or CRS object, optional\n        Coordinate reference system to project all datasets to\n\n    Returns:\n    --------\n    GeoDataFrame\n        Target geometries with interpolated numeric values\n    \"\"\"\n\n    # Input validation\n    if not all(isinstance(gdf, gpd.GeoDataFrame) for gdf in [from_gdf, to_gdf, weights_gdf]):\n        raise ValueError(\"All inputs must be GeoDataFrames\")\n\n    # Make copies to avoid modifying originals\n    from_gdf = from_gdf.copy()\n    to_gdf = to_gdf.copy()\n    weights_gdf = weights_gdf.copy()\n\n    # Set CRS if provided\n    if crs:\n        from_gdf = from_gdf.to_crs(crs)\n        to_gdf = to_gdf.to_crs(crs)\n        weights_gdf = weights_gdf.to_crs(crs)\n\n    # Check CRS consistency\n    if not (from_gdf.crs == to_gdf.crs == weights_gdf.crs):\n        raise ValueError(\"All inputs must have the same CRS\")\n\n    # Handle to_id\n    if to_id is None:\n        to_id = 'id'\n        to_gdf[to_id] = to_gdf.index.astype(str)\n\n    # Remove conflicting columns\n    if to_id in from_gdf.columns:\n        from_gdf = from_gdf.drop(columns=[to_id])\n\n    # Create unique from_id\n    from_id = 'from_id'\n    from_gdf[from_id] = from_gdf.index.astype(str)\n\n    # Handle weight column\n    if weight_column is None:\n        weight_column = 'interpolation_weight'\n        weights_gdf[weight_column] = 1.0\n    else:\n        # Rename to avoid conflicts\n        weights_gdf['interpolation_weight'] = weights_gdf[weight_column]\n        weight_column = 'interpolation_weight'\n\n    # Convert weights to points if needed\n    if weights_gdf.geometry.geom_type.iloc[0] in ['Polygon', 'MultiPolygon']:\n        if weight_placement == 'surface':\n            weights_gdf = weights_gdf.copy()\n            weights_gdf.geometry = weights_gdf.geometry.representative_point()\n        elif weight_placement == 'centroid':\n            weights_gdf = weights_gdf.copy()\n            weights_gdf.geometry = weights_gdf.geometry.centroid\n        else:\n            raise ValueError(\"weight_placement must be 'surface' or 'centroid'\")\n\n    # Keep only weight column and geometry\n    weight_points = weights_gdf[[weight_column, 'geometry']].copy()\n\n    # Calculate denominators (total weights per source geometry)\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        source_weights = gpd.sjoin(from_gdf, weight_points, how='left', predicate='contains')\n\n    denominators = (source_weights.groupby(from_id)[weight_column]\n                   .sum()\n                   .reset_index()\n                   .rename(columns={weight_column: 'weight_total'}))\n\n    # Calculate intersections between from and to\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        intersections = gpd.overlay(from_gdf, to_gdf, how='intersection')\n\n    # Filter to keep only polygon intersections\n    intersections = intersections[intersections.geometry.geom_type.isin(['Polygon', 'MultiPolygon', 'GeometryCollection'])]\n\n    if len(intersections) == 0:\n        raise ValueError(\"No valid polygon intersections found between source and target geometries\")\n\n    # Add intersection ID\n    intersections['intersection_id'] = range(len(intersections))\n\n    # Spatial join intersections with weight points to get weights within each intersection\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', category=UserWarning)\n        intersection_weights = gpd.sjoin(intersections, weight_points, how='left', predicate='contains')\n\n    # Calculate intersection values (sum of weights per intersection)\n    intersection_values = (intersection_weights.groupby('intersection_id')[weight_column]\n                         .sum()\n                         .reset_index()\n                         .rename(columns={weight_column: 'intersection_value'}))\n\n    # Merge back to intersections and keep only unique intersections\n    intersections = intersections.merge(intersection_values, on='intersection_id', how='left')\n    intersections['intersection_value'] = intersections['intersection_value'].fillna(0)\n\n    # Remove duplicates created by the spatial join\n    intersections = intersections.drop_duplicates(subset='intersection_id')\n\n    # Merge with denominators to calculate weight coefficients\n    intersections = intersections.merge(denominators, on=from_id, how='left')\n    intersections['weight_total'] = intersections['weight_total'].fillna(1)\n\n    # Calculate weight coefficients (intersection weight / total weight in source)\n    intersections.loc[intersections['weight_total'] &gt; 0, 'weight_coef'] = (\n        intersections['intersection_value'] / intersections['weight_total']\n    )\n    intersections['weight_coef'] = intersections['weight_coef'].fillna(0)\n\n    # Get numeric columns from source data\n    numeric_cols = from_gdf.select_dtypes(include=[np.number]).columns\n    # Remove ID columns\n    numeric_cols = [col for col in numeric_cols if col not in [from_id]]\n\n    # Prepare intersection data for interpolation\n    intersection_data = intersections[[from_id, to_id, 'weight_coef'] + list(numeric_cols)].copy()\n\n    if extensive:\n        # For extensive variables: multiply by weight coefficient, then sum by target\n        for col in numeric_cols:\n            intersection_data[col] = intersection_data[col] * intersection_data['weight_coef']\n\n        interpolated = (intersection_data.groupby(to_id)[list(numeric_cols)]\n                       .sum()\n                       .reset_index())\n    else:\n        # For intensive variables: weighted average\n        interpolated_data = []\n        for target_id in intersection_data[to_id].unique():\n            target_data = intersection_data[intersection_data[to_id] == target_id]\n            if len(target_data) &gt; 0 and target_data['weight_coef'].sum() &gt; 0:\n                weighted_vals = {}\n                for col in numeric_cols:\n                    weighted_vals[col] = (target_data[col] * target_data['weight_coef']).sum() / target_data['weight_coef'].sum()\n                weighted_vals[to_id] = target_id\n                interpolated_data.append(weighted_vals)\n\n        interpolated = pd.DataFrame(interpolated_data)\n\n    # Merge with target geometries\n    result = to_gdf[[to_id, 'geometry']].merge(interpolated, on=to_id, how='left')\n\n    # Fill NaN values with 0 for missing interpolations\n    for col in numeric_cols:\n        if col in result.columns:\n            result[col] = result[col].fillna(0)\n\n    return result\n\n\n\n\nMedian Income Estimation Function\nWhen detailed income distribution data is unavailable at the Census block level, we need to estimate median income from the income bracket counts available in the ACS. This function implements the cumulative distribution method, which:\n\nCalculates the cumulative sum of households across income brackets\nIdentifies the bracket containing the 50th percentile (median)\nReturns the midpoint of that bracket as the estimated median\n\nThis provides a reasonable approximation when direct median values are suppressed or unavailable due to data quality concerns.\n\n\nShow the code\ndef estimate_median(row, income_columns, midpoints):\n    \"\"\"\n    Find median income from household income distribution using cumulative sum method.\n\n    Parameters:\n    -----------\n    row : DataFrame row\n        Row with household counts by income bracket\n    income_columns : list\n        List of column names with HH counts (HH_LT_10K, HH_10_15K, etc.)\n    midpoints : array\n        Array of income bracket midpoints\n\n    Returns:\n    --------\n    float\n        Estimated median income\n    \"\"\"\n    total_hh = row['HH_TOTAL']\n\n    # Handle edge cases\n    if pd.isna(total_hh) or total_hh == 0:\n        return np.nan\n\n    cumsum = 0\n    for col, midpoint in zip(income_columns, midpoints):\n        # Handle missing values in income brackets\n        hh_count = row[col] if pd.notna(row[col]) else 0\n        cumsum += hh_count\n\n        if cumsum &gt; total_hh / 2:\n            return midpoint\n\n    return 300000  # Default for highest bracket"
  },
  {
    "objectID": "index.html#raw-data-sources",
    "href": "index.html#raw-data-sources",
    "title": "TAZ & Regional Median Income",
    "section": "4 Raw Data Sources",
    "text": "4 Raw Data Sources\n\nTransportation Analysis Zone\nLoad the statewide TAZ shapefile, which contains the geographic boundaries for all transportation analysis zones in Utah. These zones are used in transportation modeling and planning. The data is reprojected to our standard coordinate system and ID columns are formatted as integers for consistency.\n\n\nShow the code\ngdf_TAZ = gpd.read_file(\n  r\"_data/TAZ/Statewide_TAZ.zip\"\n).to_crs(PROJECT_CRS)\n\n# Refine the format for ID columns\ngdf_TAZ[\"CO_TAZID\"] = gdf_TAZ[\"CO_TAZID\"].astype(int)\ngdf_TAZ[\"SUBAREAID\"] = gdf_TAZ[\"SUBAREAID\"].astype(int)\ngdf_TAZ[\"CO_FIPS\"] = gdf_TAZ[\"CO_FIPS\"].astype(int)\n\nprint(f\"Total TAZs: {len(gdf_TAZ)}\")\ngdf_TAZ.head()\n\n\nTotal TAZs: 9891\n\n\n\n\n\n\n\n\n\nTAZID\nSA_TAZID\nCO_TAZID\nSUBAREAID\nACRES\nDEVACRES\nX\nY\nADJ_XY\nCO_FIPS\n...\nDMED_NAME\nDISTSML\nDSML_NAME\nCITY_NAME\nIMX_GENRL\nIMX_COALIX\nIMX_OLGAIX\nIMX_OLGAXI\nIMX_PETRIX\ngeometry\n\n\n\n\n0\n7106\n1\n50001\n2\n490.0959\n490.0959\n420115.925110\n4.649425e+06\n0\n5\n...\nTrenton, Cornish, Lewiston Are\n97\nCornish\nCornish\n0.0\n0.0\n0.0\n0.0\n0.0\nPOLYGON ((1516749.986 7897230.75, 1516680.522 ...\n\n\n1\n7107\n2\n50002\n2\n600.4072\n564.3828\n421423.818771\n4.648649e+06\n0\n5\n...\nTrenton, Cornish, Lewiston Are\n97\nCornish\nCornish\n0.0\n0.0\n0.0\n0.0\n0.0\nPOLYGON ((1516749.986 7897230.75, 1519378.33 7...\n\n\n2\n7108\n3\n50003\n2\n310.0336\n263.5285\n422350.576244\n4.649540e+06\n0\n5\n...\nTrenton, Cornish, Lewiston Are\n97\nCornish\nCornish\n0.0\n0.0\n0.0\n0.0\n0.0\nPOLYGON ((1519378.33 7897255.589, 1520452.919 ...\n\n\n3\n7109\n4\n50004\n2\n324.9306\n324.9306\n419600.980676\n4.648144e+06\n0\n5\n...\nTrenton, Cornish, Lewiston Are\n97\nCornish\nCornish\n0.0\n0.0\n0.0\n0.0\n0.0\nPOLYGON ((1514097.051 7889461.259, 1513855.586...\n\n\n4\n7110\n5\n50005\n2\n269.1452\n269.1452\n420521.535873\n4.648139e+06\n0\n5\n...\nTrenton, Cornish, Lewiston Are\n97\nCornish\nCornish\n0.0\n0.0\n0.0\n0.0\n0.0\nPOLYGON ((1516834.448 7889652.216, 1516853.139...\n\n\n\n\n5 rows × 26 columns\n\n\n\nThe TAZ dataset includes 9891 zones across Utah, covering both metropolitan planning organization (MPO) areas and rural regions.\n\n\nShow the code\n# Preview the TAZ in an interactive map\ngdf_TAZ.explore(column=\"CO_FIPS\", cmap=\"tab20\", legend=True)\n\n\n\n\nConsumer Price Index\nThe Consumer Price Index (CPI-U-RS) from the Bureau of Labor Statistics provides inflation adjustment factors. This will be useful for comparing income values across different time periods or adjusting historical data to current dollars. The research series (R-CPI-U-RS) is particularly appropriate for longitudinal analysis as it uses consistent methodology throughout the time series.\nDownload and load the CPI data directly from the BLS website. The data includes annual average CPI values that can be used to adjust income figures for inflation.\nData Source: Consumer Price Index for All Urban Consumers (CPI-U) [Source: Bureau of Labor Statistics]\n\n\nShow the code\n# Set file path and URL for CPI data\nfilepath_cpi = Path(\"_data/bls/r-cpi-u-rs-allitems.xlsx\")\nurl_cpi = \"https://www.bls.gov/cpi/research-series/r-cpi-u-rs-allitems.xlsx\"\n\n# Ensure the file exists, Download if not\nfetch_excel(path=filepath_cpi, url=url_cpi)\n\n# Read Excel file\ndf_CPI = pd.read_excel(\n    filepath_cpi,\n    sheet_name=\"All items\",\n    usecols=\"A:N\",\n    skiprows=5,\n    engine='openpyxl'\n)\n\ndf_CPI\n\n\n\n\n\n\n\n\n\nYEAR\nJAN\nFEB\nMAR\nAPR\nMAY\nJUNE\nJULY\nAUG\nSEP\nOCT\nNOV\nDEC\nAVG\n\n\n\n\n0\n1977\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n100.0\nNaN\n\n\n1\n1978\n100.5\n101.1\n101.8\n102.7\n103.6\n104.5\n105.0\n105.5\n106.1\n106.7\n107.3\n107.8\n104.4\n\n\n2\n1979\n108.7\n109.7\n110.7\n111.8\n113.0\n114.1\n115.1\n116.0\n117.1\n117.9\n118.5\n119.5\n114.3\n\n\n3\n1980\n120.8\n122.4\n123.8\n124.7\n125.7\n126.7\n127.5\n128.6\n129.9\n130.7\n131.5\n132.4\n127.1\n\n\n4\n1981\n133.6\n135.2\n136.3\n137.1\n137.9\n138.7\n139.7\n140.7\n141.8\n142.4\n142.9\n143.4\n139.1\n\n\n5\n1982\n144.2\n144.7\n144.9\n145.0\n146.1\n147.5\n148.5\n148.8\n149.5\n150.2\n150.5\n150.6\n147.5\n\n\n6\n1983\n151.0\n151.1\n151.2\n152.4\n153.2\n153.7\n154.3\n154.8\n155.6\n156.0\n156.1\n156.3\n153.8\n\n\n7\n1984\n157.2\n158.0\n158.3\n159.0\n159.5\n159.9\n160.4\n161.1\n161.8\n162.2\n162.2\n162.3\n160.2\n\n\n8\n1985\n162.5\n163.2\n163.9\n164.6\n165.2\n165.6\n165.9\n166.2\n166.8\n167.2\n167.7\n168.1\n165.6\n\n\n9\n1986\n168.6\n168.1\n167.3\n166.9\n167.4\n168.2\n168.2\n168.5\n169.4\n169.5\n169.5\n169.6\n168.4\n\n\n10\n1987\n170.6\n171.3\n172.0\n172.9\n173.4\n174.0\n174.3\n175.3\n176.1\n176.5\n176.6\n176.5\n174.1\n\n\n11\n1988\n177.0\n177.3\n178.0\n178.9\n179.5\n180.1\n180.8\n181.6\n182.7\n183.2\n183.3\n183.4\n180.5\n\n\n12\n1989\n184.3\n184.9\n185.9\n187.2\n188.1\n188.5\n189.0\n189.2\n189.8\n190.6\n190.9\n191.1\n188.3\n\n\n13\n1990\n193.0\n193.9\n194.9\n195.2\n195.5\n196.5\n197.3\n199.0\n200.6\n201.7\n202.0\n202.0\n197.6\n\n\n14\n1991\n202.9\n203.1\n203.3\n203.5\n204.1\n204.5\n204.7\n205.2\n206.1\n206.2\n206.7\n206.8\n204.8\n\n\n15\n1992\n207.2\n207.7\n208.6\n209.0\n209.3\n209.7\n210.1\n210.6\n211.2\n211.8\n212.1\n211.9\n209.9\n\n\n16\n1993\n212.6\n213.3\n214.0\n214.5\n215.0\n215.2\n215.3\n215.7\n216.0\n216.7\n216.9\n216.7\n215.2\n\n\n17\n1994\n217.1\n217.6\n218.4\n218.6\n218.9\n219.5\n220.0\n220.7\n221.1\n221.2\n221.5\n221.4\n219.7\n\n\n18\n1995\n222.2\n222.9\n223.6\n224.3\n224.7\n225.2\n225.3\n225.7\n226.1\n226.7\n226.5\n226.4\n225.0\n\n\n19\n1996\n227.5\n228.3\n229.4\n230.2\n230.8\n230.9\n231.3\n231.5\n232.3\n233.0\n233.4\n233.4\n231.0\n\n\n20\n1997\n234.1\n234.7\n235.2\n235.5\n235.4\n235.8\n235.9\n236.3\n237.1\n237.5\n237.4\n237.0\n236.0\n\n\n21\n1998\n237.4\n237.8\n238.1\n238.6\n239.0\n239.1\n239.4\n239.7\n240.0\n240.5\n240.5\n240.3\n239.2\n\n\n22\n1999\n240.9\n241.2\n241.9\n243.6\n243.6\n243.7\n244.4\n245.1\n246.2\n246.7\n246.8\n246.8\n244.2\n\n\n23\n2000\n247.6\n249.1\n251.1\n251.2\n251.4\n252.9\n253.4\n253.4\n254.8\n255.1\n255.3\n255.1\n252.5\n\n\n24\n2001\n256.8\n257.9\n258.5\n259.5\n260.5\n261.1\n260.3\n260.4\n261.4\n260.6\n260.1\n259.1\n259.7\n\n\n25\n2002\n259.8\n260.8\n262.2\n263.7\n263.6\n263.9\n264.1\n265.0\n265.4\n265.9\n265.9\n265.3\n263.8\n\n\n26\n2003\n266.4\n268.6\n270.2\n269.5\n269.1\n269.5\n269.7\n270.7\n271.6\n271.4\n270.6\n270.3\n269.8\n\n\n27\n2004\n271.7\n273.2\n274.9\n275.8\n277.3\n278.2\n277.8\n277.9\n278.5\n280.0\n280.2\n279.1\n277.0\n\n\n28\n2005\n279.6\n281.3\n283.5\n285.4\n285.2\n285.2\n286.5\n288.0\n291.5\n292.2\n289.8\n288.6\n286.4\n\n\n29\n2006\n290.8\n291.4\n293.1\n295.5\n296.9\n297.6\n298.4\n299.1\n297.6\n296.0\n295.6\n296.0\n295.7\n\n\n30\n2007\n296.9\n298.5\n301.2\n303.1\n305.0\n305.6\n305.5\n304.9\n305.8\n306.4\n308.3\n308.1\n304.1\n\n\n31\n2008\n309.6\n310.5\n313.2\n315.1\n317.7\n320.9\n322.6\n321.3\n320.9\n317.6\n311.6\n308.3\n315.8\n\n\n32\n2009\n309.7\n311.2\n312.0\n312.8\n313.7\n316.4\n315.8\n316.6\n316.8\n317.1\n317.3\n316.7\n314.7\n\n\n33\n2010\n317.8\n317.9\n319.2\n319.8\n320.0\n319.7\n319.8\n320.2\n320.4\n320.8\n320.9\n321.5\n319.8\n\n\n34\n2011\n323.0\n324.6\n327.8\n329.9\n331.5\n331.1\n331.4\n332.3\n332.8\n332.2\n331.9\n331.1\n330.0\n\n\n35\n2012\n332.6\n334.0\n336.6\n337.6\n337.2\n336.7\n336.2\n338.1\n339.6\n339.5\n337.9\n337.0\n336.9\n\n\n36\n2013\n338.0\n340.8\n341.7\n341.3\n341.9\n342.8\n342.9\n343.3\n343.8\n342.9\n342.2\n342.2\n342.0\n\n\n37\n2014\n343.5\n344.8\n347.0\n348.2\n349.4\n350.1\n349.9\n349.3\n349.6\n348.7\n346.9\n344.9\n347.7\n\n\n38\n2015\n343.4\n344.9\n346.9\n347.7\n349.4\n350.7\n350.7\n350.2\n349.7\n349.5\n348.8\n347.6\n348.3\n\n\n39\n2016\n348.2\n348.5\n350.0\n351.7\n353.1\n354.2\n353.7\n354.0\n354.8\n355.3\n354.7\n354.9\n352.8\n\n\n40\n2017\n356.9\n358.0\n358.3\n359.4\n359.7\n360.0\n359.8\n360.9\n362.8\n362.5\n362.6\n362.3\n360.3\n\n\n41\n2018\n364.3\n366.0\n366.7\n368.3\n369.8\n370.3\n370.4\n370.7\n371.1\n371.8\n370.6\n369.3\n369.1\n\n\n42\n2019\n370.0\n371.5\n373.6\n375.6\n376.4\n376.5\n377.2\n377.1\n377.5\n378.4\n378.2\n377.8\n375.8\n\n\n43\n2020\n379.2\n380.2\n379.5\n377.2\n377.2\n379.4\n381.3\n382.6\n383.1\n383.2\n382.9\n383.2\n380.8\n\n\n44\n2021\n384.9\n387.1\n390.0\n393.2\n396.7\n400.5\n402.4\n403.1\n404.2\n407.6\n409.5\n410.8\n399.2\n\n\n45\n2022\n414.3\n418.2\n423.9\n426.3\n431.0\n436.9\n436.8\n436.7\n437.6\n439.4\n439.0\n437.6\n431.5\n\n\n46\n2023\n441.1\n443.6\n445.0\n447.3\n448.4\n449.9\n450.7\n452.7\n453.8\n453.6\n452.7\n452.3\n449.3\n\n\n47\n2024\n454.7\n457.6\n460.5\n462.3\n463.1\n463.2\n463.8\n464.1\n464.9\n465.4\n465.2\n465.3\n462.5\n\n\n\n\n\n\n\n\n\nAmerican Community Survey (ACS) 5-Year Estimates\n\nDefine Census Variables\nDefine the specific ACS variables we need for this analysis. Table B19013 provides median household income, while Table B19001 provides the household income distribution across 16 income brackets. These brackets allow us to calculate weighted averages and estimate medians when direct values are unavailable.\n\n\nShow the code\n# Define variables to download\nacs_variables = {\n    'B19013_001E': 'HH_MED_INC',  # Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars)\n    'B19013_001M': 'HH_MED_INC_MOE',  # Margin of Error for Median Household Income\n    'B19001_001E': 'HH_TOTAL',  # Total Households\n    'B19001_002E': 'HH_LT_10K',  # Less than $10,000\n    'B19001_003E': 'HH_10_15K',  # $10,000 to $14,999\n    'B19001_004E': 'HH_15_20K',  # $15,000 to $19,999\n    'B19001_005E': 'HH_20_25K',  # $20,000 to $24,999\n    'B19001_006E': 'HH_25_30K',  # $25,000 to $29,999\n    'B19001_007E': 'HH_30_35K',  # $30,000 to $34,999\n    'B19001_008E': 'HH_35_40K',  # $35,000 to $39,999\n    'B19001_009E': 'HH_40_45K',  # $40,000 to $44,999\n    'B19001_010E': 'HH_45_50K',  # $45,000 to $49,999\n    'B19001_011E': 'HH_50_60K',  # $50,000 to $59,999\n    'B19001_012E': 'HH_60_75K',  # $60,000 to $74,999\n    'B19001_013E': 'HH_75_100K',  # $75,000 to $99,999\n    'B19001_014E': 'HH_100_125K',  # $100,000 to $124,999\n    'B19001_015E': 'HH_125_150K',  # $125,000 to $149,999\n    'B19001_016E': 'HH_150_200K',  # $150,000 to $199,999\n    'B19001_017E': 'HH_GT_200K'  # $200,000 or more\n}\n\n\n\n\nState Level Data\nRetrieve state-level income data for Utah. While we’ll primarily use smaller geographies, state-level data provides a useful benchmark for comparison and validation.\n\n\nShow the code\n# Fetch state boundaries from TIGER/Line shapefiles\ngdf_ut_bound = states(\n  year=2023,\n  cache=True\n).to_crs(PROJECT_CRS)\n\n# Filter for Utah only\ngdf_ut_bound = gdf_ut_bound[gdf_ut_bound['STATEFP'] == str(STATE_FIPS)]\n\n# Fetch Income data from ACS 5-year estimates for Utah\ndf_ut_income = get_census(\n  dataset=\"acs/acs5\",\n  year=2023,\n  variables=list(acs_variables.keys()),\n  params={\n      # \"key\": f\"{os.getenv('CENSUS_API_KEY')}\", # FIXME: This causes error\n      \"for\": f\"state:{STATE_FIPS}\"\n    },\n    return_geoid=True,\n    guess_dtypes=True\n)\n\n# Join ACS data to block group boundaries and transform CRS\ngdf_ut_income = gdf_ut_bound[['GEOID', 'STATEFP', 'NAME', 'geometry']].merge(\n    df_ut_income, on = \"GEOID\"\n).to_crs(PROJECT_CRS).rename(columns=acs_variables)\n\n# Preview data\ngdf_ut_income\n\n\n\n\n\n\n\n\n\nGEOID\nSTATEFP\nNAME\ngeometry\nHH_MED_INC\nHH_MED_INC_MOE\nHH_TOTAL\nHH_LT_10K\nHH_10_15K\nHH_15_20K\n...\nHH_35_40K\nHH_40_45K\nHH_45_50K\nHH_50_60K\nHH_60_75K\nHH_75_100K\nHH_100_125K\nHH_125_150K\nHH_150_200K\nHH_GT_200K\n\n\n\n\n0\n49\n49\nUtah\nPOLYGON ((900313.399 6302435.171, 900580.099 6...\n91750\n634\n1094896\n33918\n22999\n22352\n...\n33072\n32113\n35150\n69627\n104883\n159368\n134089\n102926\n124090\n137560\n\n\n\n\n1 rows × 23 columns\n\n\n\n\n\nCounty Level Data\nFetch county-level income data for all 29 counties in Utah. County medians serve as a fallback when block group data is unavailable or unreliable. The ACS provides relatively stable estimates at the county level due to larger sample sizes.\n\n\nShow the code\n# Fetch county boundaries from TIGER/Line shapefiles\ngdf_county_bound = counties(\n  state=STATE_FIPS,\n  year=2023,\n  cache=True\n).to_crs(PROJECT_CRS)\n\n# Fetch Income data from ACS 5-year estimates for counties in Utah\ndf_county_income = get_census(\n  dataset=\"acs/acs5\",\n  year=2023,\n  variables=list(acs_variables.keys()),\n  params={\n      # \"key\": f\"{os.getenv('CENSUS_API_KEY')}\", # FIXME: This causes error\n      \"for\": f\"county:*\",\n      \"in\": f\"state:{STATE_FIPS}\"\n    },\n    return_geoid=True,\n    guess_dtypes=True\n)\n\n# Join ACS data to county boundaries and tranform CRS\ngdf_county_income = gdf_county_bound[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(\n    df_county_income, on = \"GEOID\"\n).to_crs(PROJECT_CRS).rename(columns=acs_variables)\n\n# Preview data\nprint(f\"Total counties: {len(gdf_county_income)}\")\ngdf_county_income.head()\n\n\nTotal counties: 29\n\n\n\n\n\n\n\n\n\nGEOID\nCOUNTYFP\nNAMELSAD\ngeometry\nHH_MED_INC\nHH_MED_INC_MOE\nHH_TOTAL\nHH_LT_10K\nHH_10_15K\nHH_15_20K\n...\nHH_35_40K\nHH_40_45K\nHH_45_50K\nHH_50_60K\nHH_60_75K\nHH_75_100K\nHH_100_125K\nHH_125_150K\nHH_150_200K\nHH_GT_200K\n\n\n\n\n0\n49033\n033\nRich County\nPOLYGON ((1637136.748 7863034.219, 1637160.731...\n76875\n7743\n817\n15\n23\n32\n...\n20\n21\n11\n63\n91\n160\n125\n19\n61\n60\n\n\n1\n49005\n005\nCache County\nPOLYGON ((1459262.09 7896818.424, 1459327.308 ...\n78292\n2156\n43118\n1271\n930\n954\n...\n1870\n1727\n1567\n3248\n4487\n6638\n5606\n3464\n3047\n3667\n\n\n2\n49013\n013\nDuchesne County\nPOLYGON ((1806607.907 7329761.438, 1806600.198...\n74738\n3589\n6714\n231\n227\n260\n...\n200\n299\n211\n524\n640\n940\n883\n539\n491\n491\n\n\n3\n49011\n011\nDavis County\nPOLYGON ((1499977.044 7504652.389, 1498947.029...\n108058\n2051\n114674\n2575\n1654\n1829\n...\n2565\n2316\n2696\n6301\n10134\n16154\n15542\n12459\n16209\n18090\n\n\n4\n49003\n003\nBox Elder County\nPOLYGON ((1466489.48 7656413.112, 1465439.601 ...\n77865\n3036\n19150\n869\n676\n365\n...\n612\n536\n859\n1321\n2552\n3147\n2298\n1580\n1743\n1184\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n\nShow the code\n# Preview the date in an interactive map\ngdf_county_income.explore(column=\"HH_MED_INC\", cmap=\"YlGnBu\", legend=True)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe Coefficient of Variation (CV) measures data reliability by comparing the margin of error to the estimate itself. Values below 15% are generally considered reliable, 15-30% should be used with caution, and values above 30% are considered unreliable. This check helps identify counties where data quality may be a concern.\n\n\nShow the code\n# Data Validity Check: Coefficient of Variation (CV) for Median Household Income\ngdf_county_income['HH_MED_INC_CV'] = (\n  gdf_county_income['HH_MED_INC_MOE'] / 1.645\n) / gdf_county_income['HH_MED_INC'] * 100\n\ngdf_county_income[['NAMELSAD', 'HH_MED_INC', 'HH_MED_INC_MOE', 'HH_MED_INC_CV']].sort_values(by='HH_MED_INC_CV', ascending=False).head(10)\n\n\n\n\n\n\n\n\n\nNAMELSAD\nHH_MED_INC\nHH_MED_INC_MOE\nHH_MED_INC_CV\n\n\n\n\n16\nDaggett County\n58750\n17147\n17.742482\n\n\n13\nPiute County\n44650\n7643\n10.405824\n\n\n17\nBeaver County\n85603\n13902\n9.872392\n\n\n8\nKane County\n75000\n11394\n9.235258\n\n\n11\nGarfield County\n61688\n8586\n8.461051\n\n\n27\nGrand County\n62521\n8080\n7.856327\n\n\n20\nWayne County\n70074\n9056\n7.856219\n\n\n25\nSan Juan County\n54890\n6962\n7.710364\n\n\n0\nRich County\n76875\n7743\n6.122915\n\n\n21\nJuab County\n89803\n8703\n5.891315\n\n\n\n\n\n\n\n\n\nBlock Group Level Data\nDownload income data for all census block groups in Utah. Block groups are the smallest geography for which the ACS publishes detailed income data. Each block group typically contains 600-3,000 people.\n\n\nShow the code\n# Fetch block group boundaries from TIGER/Line shapefiles\ngdf_bg_bound = block_groups(\n  state=STATE_FIPS,\n  year=2023,\n  cache=True\n).to_crs(PROJECT_CRS)\n\n# Fetch Income data from ACS 5-year estimates for block groups in Utah\ndf_bg_income = get_census(\n  dataset=\"acs/acs5\",\n  year=2023,\n  variables=list(acs_variables.keys()),\n  params={\n      # \"key\": f\"{os.getenv('CENSUS_API_KEY')}\", # FIXME: This causes error\n      \"for\": f\"block group:*\",\n      \"in\": f\"state:{STATE_FIPS} county:*\"\n    },\n    return_geoid=True,\n    guess_dtypes=True\n)\n\n# Join ACS data to block group boundaries and transform CRS\ngdf_bg_income = gdf_bg_bound[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(\n    df_bg_income, on = \"GEOID\"\n).to_crs(PROJECT_CRS).rename(columns=acs_variables)\n\n# preview data\nprint(f\"Total block groups: {len(gdf_bg_income)}\")\ngdf_bg_income.head(10)\n\n\nTotal block groups: 2020\n\n\n\n\n\n\n\n\n\nGEOID\nCOUNTYFP\nNAMELSAD\ngeometry\nHH_MED_INC\nHH_MED_INC_MOE\nHH_TOTAL\nHH_LT_10K\nHH_10_15K\nHH_15_20K\n...\nHH_35_40K\nHH_40_45K\nHH_45_50K\nHH_50_60K\nHH_60_75K\nHH_75_100K\nHH_100_125K\nHH_125_150K\nHH_150_200K\nHH_GT_200K\n\n\n\n\n0\n490039603014\n003\nBlock Group 4\nPOLYGON ((1457672.386 7795318.902, 1457692.203...\n95313.0\n47823.0\n373\n0\n0\n0\n...\n16\n13\n17\n23\n61\n59\n45\n32\n66\n33\n\n\n1\n490039603021\n003\nBlock Group 1\nPOLYGON ((1454296.557 7798327.645, 1454324.964...\n72872.0\n20284.0\n801\n0\n15\n0\n...\n22\n12\n117\n0\n146\n227\n54\n0\n36\n12\n\n\n2\n490039603011\n003\nBlock Group 1\nPOLYGON ((1458913.147 7790335.904, 1458913.726...\n99519.0\n29498.0\n367\n3\n3\n6\n...\n2\n21\n11\n12\n36\n84\n37\n39\n52\n53\n\n\n3\n490039603012\n003\nBlock Group 2\nPOLYGON ((1457125.723 7786638.014, 1457132.453...\n64970.0\n16467.0\n505\n0\n69\n0\n...\n0\n0\n15\n23\n158\n64\n69\n39\n0\n0\n\n\n4\n490039603013\n003\nBlock Group 3\nPOLYGON ((1457448.445 7792187.142, 1457460.447...\n71507.0\n23375.0\n313\n0\n0\n0\n...\n20\n0\n0\n0\n113\n0\n81\n17\n0\n18\n\n\n5\n490039603022\n003\nBlock Group 2\nPOLYGON ((1447664.85 7792861.347, 1447700.369 ...\n75333.0\n34885.0\n386\n0\n14\n50\n...\n0\n14\n0\n37\n76\n54\n9\n73\n39\n20\n\n\n6\n490039603023\n003\nBlock Group 3\nPOLYGON ((1440417.061 7797111.403, 1440454.858...\n70511.0\n14091.0\n515\n52\n14\n0\n...\n53\n0\n22\n65\n173\n73\n24\n0\n0\n39\n\n\n7\n490439643053\n043\nBlock Group 3\nPOLYGON ((1611350.82 7412360.772, 1611358.374 ...\n250001.0\nNaN\n127\n0\n0\n0\n...\n0\n0\n7\n0\n0\n12\n9\n4\n5\n79\n\n\n8\n490039603024\n003\nBlock Group 4\nPOLYGON ((1454390.221 7792981.289, 1454412.898...\n56402.0\n8388.0\n444\n0\n20\n10\n...\n48\n17\n18\n138\n60\n33\n44\n18\n38\n0\n\n\n9\n490439644021\n043\nBlock Group 1\nPOLYGON ((1637685.712 7393688.213, 1637745.164...\nNaN\nNaN\n148\n0\n0\n0\n...\n0\n61\n0\n0\n21\n0\n23\n0\n0\n43\n\n\n\n\n10 rows × 23 columns\n\n\n\n\n\nShow the code\n# Preview the date in an interactive map\ngdf_bg_income.explore(column=\"HH_MED_INC\", cmap=\"YlGnBu\", legend=True)\n\n\nBlock groups with null median income values or extreme values (≥$250,000) require special handling. These often occur in areas with small populations, high-income enclaves, or data quality issues. For these cases, we’ll use either estimated medians from the income distribution or fall back to county values.\n\n\nShow the code\n# Data Validity Check\ngdf_bg_income[\n    gdf_bg_income[\"HH_MED_INC\"].isna() | (gdf_bg_income[\"HH_MED_INC\"] &gt;= 250000)\n]\n\n\n\n\n\n\n\n\n\nGEOID\nCOUNTYFP\nNAMELSAD\ngeometry\nHH_MED_INC\nHH_MED_INC_MOE\nHH_TOTAL\nHH_LT_10K\nHH_10_15K\nHH_15_20K\n...\nHH_35_40K\nHH_40_45K\nHH_45_50K\nHH_50_60K\nHH_60_75K\nHH_75_100K\nHH_100_125K\nHH_125_150K\nHH_150_200K\nHH_GT_200K\n\n\n\n\n7\n490439643053\n043\nBlock Group 3\nPOLYGON ((1611350.82 7412360.772, 1611358.374 ...\n250001.0\nNaN\n127\n0\n0\n0\n...\n0\n0\n7\n0\n0\n12\n9\n4\n5\n79\n\n\n9\n490439644021\n043\nBlock Group 1\nPOLYGON ((1637685.712 7393688.213, 1637745.164...\nNaN\nNaN\n148\n0\n0\n0\n...\n0\n61\n0\n0\n21\n0\n23\n0\n0\n43\n\n\n11\n490439644023\n043\nBlock Group 3\nPOLYGON ((1644362.851 7410618.596, 1644376.438...\nNaN\nNaN\n212\n10\n0\n0\n...\n12\n0\n13\n68\n0\n0\n0\n25\n32\n28\n\n\n17\n490111257022\n011\nBlock Group 2\nPOLYGON ((1495452.696 7576157.753, 1495455.973...\nNaN\nNaN\n624\n96\n103\n50\n...\n20\n0\n8\n29\n113\n109\n25\n9\n20\n0\n\n\n36\n490451308001\n045\nBlock Group 1\nPOLYGON ((1374205.571 7390229.263, 1374207.902...\nNaN\nNaN\n294\n0\n20\n0\n...\n60\n0\n0\n0\n27\n56\n29\n0\n0\n91\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1927\n490490022091\n049\nBlock Group 1\nPOLYGON ((1568307.053 7287618.341, 1569785.588...\nNaN\nNaN\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1964\n490499806001\n049\nBlock Group 1\nPOLYGON ((1573655 7254082.838, 1573794.857 725...\nNaN\nNaN\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1981\n490490014032\n049\nBlock Group 2\nPOLYGON ((1592314.617 7262270.71, 1592370.524 ...\nNaN\nNaN\n46\n0\n12\n17\n...\n0\n0\n0\n0\n7\n10\n0\n0\n0\n0\n\n\n1985\n490490017013\n049\nBlock Group 3\nPOLYGON ((1600460.288 7259733.736, 1600492.384...\nNaN\nNaN\n89\n0\n0\n0\n...\n0\n0\n0\n0\n0\n23\n0\n9\n18\n1\n\n\n1989\n490490027024\n049\nBlock Group 4\nPOLYGON ((1605212.125 7249518.913, 1605235.656...\nNaN\nNaN\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n73 rows × 23 columns\n\n\n\n\n\nDecennial Census 2020 - Census Blocks\nCensus blocks are the smallest geographic unit used by the Census Bureau, typically containing just a few dozen housing units. While the ACS doesn’t provide income data at this level, the 2020 Decennial Census provides precise household counts that we can use as weights for spatial interpolation.\n\n\nShow the code\n# Define variables\ndec_variables = {\n    'H9_001N': 'HH20'  # Total Number of Households\n}\n\n# Get census block geometries\ngdf_block_bound = blocks(\n    state=STATE_FIPS,\n    year=2020,\n    cache=True\n)\n\n# Download household counts\ndf_block_hh = get_census(\n    dataset=\"dec/dhc\",\n    year=2020,\n    variables=list(dec_variables.keys()),\n    params={\n        \"for\": f\"block:*\",\n        \"in\": f\"state:{STATE_FIPS} county:*\"\n    },\n    return_geoid=True,\n    guess_dtypes=True,\n)\n\n# Join and rename\ngdf_block_hh = gdf_block_bound[['GEOID20', 'geometry']].merge(\n    df_block_hh, left_on=\"GEOID20\", right_on=\"GEOID\"\n).to_crs(PROJECT_CRS).rename(columns=dec_variables)[['GEOID', 'HH20', 'geometry']]\n\nprint(f\"Total census blocks: {len(gdf_block_hh)}\")\nprint(f\"Blocks with households: {(gdf_block_hh['HH20'] &gt; 0).sum()}\")\ngdf_block_hh.head()\n\n\nTotal census blocks: 71207\nBlocks with households: 44702\n\n\n\n\n\n\n\n\n\nGEOID\nHH20\ngeometry\n\n\n\n\n0\n490351143041007\n18\nPOLYGON ((1498087.949 7387694.013, 1498091.062...\n\n\n1\n490351130221042\n0\nPOLYGON ((1502208.892 7368252.412, 1502215.882...\n\n\n2\n490351128302015\n53\nPOLYGON ((1533128.823 7346280.041, 1533134.077...\n\n\n3\n490351139091031\n26\nPOLYGON ((1482871.57 7388668.635, 1482871.675 ...\n\n\n4\n490351128134002\n9\nPOLYGON ((1548140.763 7367459.474, 1548156.067...\n\n\n\n\n\n\n\nOut of 71207, total census blocks in Utah, only 44702 have households. Many blocks represent unpopulated areas like parks, water bodies, or commercial/industrial zones."
  },
  {
    "objectID": "index.html#lookup-tables",
    "href": "index.html#lookup-tables",
    "title": "TAZ & Regional Median Income",
    "section": "5 Lookup Tables",
    "text": "5 Lookup Tables\n\nIncome Category Lookup\nCreate a reference table defining the income brackets used in ACS Table B19001. Each bracket has a lower and upper limit, and we calculate midpoints for median estimation. The highest bracket ($200,000+) uses $300,000 as a reasonable midpoint based on income distribution patterns.\n\n\nShow the code\nlookup_hhinc = pd.DataFrame({\n  \"Income Category\": [\n    \"HH_LT_10K\", \"HH_10_15K\", \"HH_15_20K\", \"HH_20_25K\", \"HH_25_30K\", \"HH_30_35K\",\n    \"HH_35_40K\", \"HH_40_45K\", \"HH_45_50K\", \"HH_50_60K\", \"HH_60_75K\",\n    \"HH_75_100K\", \"HH_100_125K\", \"HH_125_150K\", \"HH_150_200K\", \"HH_GT_200K\"\n  ],\n  \"Lower Limit\": [\n    0, 10000, 15000, 20000, 25000, 30000,\n    35000, 40000, 45000, 50000, 60000,\n    75000, 100000, 125000, 150000, 200000\n  ],\n  \"Upper Limit\": [\n    9999, 14999, 19999, 24999, 29999, 34999,\n    39999, 44999, 49999, 59999, 74999,\n    99999, 124999, 149999, 199999, np.inf\n  ]\n})\n\n# Compute midpoint and round it\nlookup_hhinc['Midpoint'] = (\n  (lookup_hhinc['Lower Limit'] + lookup_hhinc['Upper Limit']) / 2\n).round()\n\n# Replace infinite midpoint (last category) with 300000\nlookup_hhinc.loc[np.isinf(lookup_hhinc[\"Upper Limit\"]), \"Midpoint\"] = 300000\n\nlookup_hhinc\n\n\n\n\n\n\n\n\n\nIncome Category\nLower Limit\nUpper Limit\nMidpoint\n\n\n\n\n0\nHH_LT_10K\n0\n9999.0\n5000.0\n\n\n1\nHH_10_15K\n10000\n14999.0\n12500.0\n\n\n2\nHH_15_20K\n15000\n19999.0\n17500.0\n\n\n3\nHH_20_25K\n20000\n24999.0\n22500.0\n\n\n4\nHH_25_30K\n25000\n29999.0\n27500.0\n\n\n5\nHH_30_35K\n30000\n34999.0\n32500.0\n\n\n6\nHH_35_40K\n35000\n39999.0\n37500.0\n\n\n7\nHH_40_45K\n40000\n44999.0\n42500.0\n\n\n8\nHH_45_50K\n45000\n49999.0\n47500.0\n\n\n9\nHH_50_60K\n50000\n59999.0\n55000.0\n\n\n10\nHH_60_75K\n60000\n74999.0\n67500.0\n\n\n11\nHH_75_100K\n75000\n99999.0\n87500.0\n\n\n12\nHH_100_125K\n100000\n124999.0\n112500.0\n\n\n13\nHH_125_150K\n125000\n149999.0\n137500.0\n\n\n14\nHH_150_200K\n150000\n199999.0\n175000.0\n\n\n15\nHH_GT_200K\n200000\ninf\n300000.0\n\n\n\n\n\n\n\n\n\nRegion Lookup\nDefine the model spaces (planning regions) used in transportation modeling. UDOT represents statewide planning, while the numbered regions correspond to metropolitan planning organizations and rural planning areas.\n\n\nShow the code\nsubareaid_lookup = {\n    0: 'UDOT',\n    1: 'Wasatch Front',\n    2: 'Cache',\n    3: 'Dixie',\n    4: 'Summit-Wasatch',\n    5: 'Iron'\n}\n\nsubareaid_lookup\n\n\n{0: 'UDOT',\n 1: 'Wasatch Front',\n 2: 'Cache',\n 3: 'Dixie',\n 4: 'Summit-Wasatch',\n 5: 'Iron'}"
  },
  {
    "objectID": "index.html#intermediate-data-processing",
    "href": "index.html#intermediate-data-processing",
    "title": "TAZ & Regional Median Income",
    "section": "6 Intermediate Data Processing",
    "text": "6 Intermediate Data Processing\n\nStep 1: Process Block Group Income Data\nThe ACS provides median income directly for most block groups, but some have missing or suppressed values due to small sample sizes or data quality concerns. We need a systematic approach to handle these cases while maintaining data reliability.\nApply the median estimation function to calculate income from the household distribution for each block group. This provides a fallback when the ACS-reported median is unavailable.\n\n\nShow the code\n# Get list of income bracket from lookup table\nincome_bracket_cols = lookup_hhinc[\"Income Category\"].tolist()\n\n# Get midpoint values from lookup table\nmidpoints = lookup_hhinc['Midpoint'].values\n\n# Prepare working dataset\ndf_bg_income_process = gdf_bg_income[[\n    'GEOID', 'COUNTYFP', 'NAMELSAD', 'HH_TOTAL', 'HH_MED_INC', 'HH_MED_INC_MOE',\n    *income_bracket_cols\n]].copy()\n\n# Apply the function to each row to calculate estimated median\ndf_bg_income_process['EstMedInc'] = df_bg_income_process.apply(\n    estimate_median,\n    axis=1,  # Apply function row-wise\n    income_columns=income_bracket_cols,\n    midpoints=midpoints\n)\n\nprint(\"Estimated median income calculated\")\ndf_bg_income_process.head()\n\n\nEstimated median income calculated\n\n\n\n\n\n\n\n\n\nGEOID\nCOUNTYFP\nNAMELSAD\nHH_TOTAL\nHH_MED_INC\nHH_MED_INC_MOE\nHH_LT_10K\nHH_10_15K\nHH_15_20K\nHH_20_25K\n...\nHH_40_45K\nHH_45_50K\nHH_50_60K\nHH_60_75K\nHH_75_100K\nHH_100_125K\nHH_125_150K\nHH_150_200K\nHH_GT_200K\nEstMedInc\n\n\n\n\n0\n490039603014\n003\nBlock Group 4\n373\n95313.0\n47823.0\n0\n0\n0\n0\n...\n13\n17\n23\n61\n59\n45\n32\n66\n33\n87500.0\n\n\n1\n490039603021\n003\nBlock Group 1\n801\n72872.0\n20284.0\n0\n15\n0\n0\n...\n12\n117\n0\n146\n227\n54\n0\n36\n12\n67500.0\n\n\n2\n490039603011\n003\nBlock Group 1\n367\n99519.0\n29498.0\n3\n3\n6\n2\n...\n21\n11\n12\n36\n84\n37\n39\n52\n53\n87500.0\n\n\n3\n490039603012\n003\nBlock Group 2\n505\n64970.0\n16467.0\n0\n69\n0\n0\n...\n0\n15\n23\n158\n64\n69\n39\n0\n0\n67500.0\n\n\n4\n490039603013\n003\nBlock Group 3\n313\n71507.0\n23375.0\n0\n0\n0\n0\n...\n0\n0\n0\n113\n0\n81\n17\n0\n18\n67500.0\n\n\n\n\n5 rows × 23 columns\n\n\n\nAssign quality flags to each block group based on data availability:\n\nFlag 0: High quality - ACS reported median is available and reasonable (&gt;$0 and &lt;$250,000)\nFlag 1: County fallback - No reliable data at block group level, use county median\nFlag 2: Estimated - Use calculated median from income distribution\n\nCreate the final median income column by selecting the most appropriate value based on the quality flag. This hierarchical approach prioritizes direct ACS estimates but ensures all block groups have a reasonable income value.\n\n\nShow the code\n# Assign data quality flags\nconditions = [\n    # Condition 1: Does the block group have a good ACS reported median?\n    (df_bg_income_process['HH_MED_INC'].notna()) &           # Median exists (not missing)\n    (df_bg_income_process['HH_MED_INC'] &gt; 0) &               # Median is positive\n    (df_bg_income_process['HH_MED_INC'] &lt; 250001),           # Median is reasonable (not extreme)\n\n    # Condition 2: Do we have an estimated median from distribution?\n    df_bg_income_process['EstMedInc'].notna()                # Estimated median exists\n]\n\ndf_bg_income_process['Flag'] = np.select(\n    conditions,      # List of conditions to check\n    [0, 2],         # Values to assign: 0 if condition 1 True, 2 if condition 2 True\n    default=1       # If neither condition is True, assign 1\n)\n\n# Join county median for fallback\ndf_bg_income_process = df_bg_income_process.merge(\n    gdf_county_income[['COUNTYFP', 'HH_MED_INC']].rename(columns={'HH_MED_INC': 'County_MedInc'}),\n    on='COUNTYFP',\n    how='left'\n)\n\n# Create final median income column based on flag\nmedinc_conditions = [\n    df_bg_income_process['Flag'] == 0,\n    df_bg_income_process['Flag'] == 1,\n    df_bg_income_process['Flag'] == 2\n]\n\nmedinc_choices = [\n    df_bg_income_process['HH_MED_INC'], # ACS Reported block group household Income\n    df_bg_income_process['County_MedInc'], # County fallback if none is true\n    df_bg_income_process['EstMedInc'] # Estimated median fallback if ACS reported doesnt exist\n]\n\ndf_bg_income_process['Median Income'] = np.select(medinc_conditions, medinc_choices, default=np.nan)\n\n\n\n\nShow the code\n# Create final processed dataset\ndf_bg_income_process = df_bg_income_process[[\n    'GEOID', 'COUNTYFP', 'NAMELSAD', 'HH_TOTAL', 'HH_MED_INC',\n    'EstMedInc', 'County_MedInc', 'Flag', 'Median Income'\n]].copy()\n\n# Add GEOID_BG column\ndf_bg_income_process['GEOID_BG'] = df_bg_income_process['GEOID']\n\n# Display summary\ndf_bg_income_process.head(10)\n\n\n\n\n\n\n\n\n\nGEOID\nCOUNTYFP\nNAMELSAD\nHH_TOTAL\nHH_MED_INC\nEstMedInc\nCounty_MedInc\nFlag\nMedian Income\nGEOID_BG\n\n\n\n\n0\n490039603014\n003\nBlock Group 4\n373\n95313.0\n87500.0\n77865\n0\n95313.0\n490039603014\n\n\n1\n490039603021\n003\nBlock Group 1\n801\n72872.0\n67500.0\n77865\n0\n72872.0\n490039603021\n\n\n2\n490039603011\n003\nBlock Group 1\n367\n99519.0\n87500.0\n77865\n0\n99519.0\n490039603011\n\n\n3\n490039603012\n003\nBlock Group 2\n505\n64970.0\n67500.0\n77865\n0\n64970.0\n490039603012\n\n\n4\n490039603013\n003\nBlock Group 3\n313\n71507.0\n67500.0\n77865\n0\n71507.0\n490039603013\n\n\n5\n490039603022\n003\nBlock Group 2\n386\n75333.0\n87500.0\n77865\n0\n75333.0\n490039603022\n\n\n6\n490039603023\n003\nBlock Group 3\n515\n70511.0\n67500.0\n77865\n0\n70511.0\n490039603023\n\n\n7\n490439643053\n043\nBlock Group 3\n127\n250001.0\n300000.0\n137058\n2\n300000.0\n490439643053\n\n\n8\n490039603024\n003\nBlock Group 4\n444\n56402.0\n55000.0\n77865\n0\n56402.0\n490039603024\n\n\n9\n490439644021\n043\nBlock Group 1\n148\nNaN\n67500.0\n137058\n2\n67500.0\n490439644021\n\n\n\n\n\n\n\n\n\nStep 2: Prepare Block Group Geometries with Income\nCombine the block group geometries with the processed income data. This creates a spatial dataset where each block group polygon contains its estimated median income and quality flag.\n\n\nShow the code\n# Get block group geometries with median income\ngdf_bg_with_income = gdf_bg_bound[['GEOID', 'geometry']].merge(\n    df_bg_income_process[['GEOID_BG', \"NAMELSAD\", \"HH_TOTAL\", \"Median Income\", \"Flag\"]],\n    left_on='GEOID',\n    right_on='GEOID_BG',\n    how='inner'\n).to_crs(PROJECT_CRS)\n\nprint(f\"Block groups with income data: {len(gdf_bg_with_income)}\")\ngdf_bg_with_income.head()\n\n\nBlock groups with income data: 2020\n\n\n\n\n\n\n\n\n\nGEOID\ngeometry\nGEOID_BG\nNAMELSAD\nHH_TOTAL\nMedian Income\nFlag\n\n\n\n\n0\n490039603014\nPOLYGON ((1457672.386 7795318.902, 1457692.203...\n490039603014\nBlock Group 4\n373\n95313.0\n0\n\n\n1\n490039603021\nPOLYGON ((1454296.557 7798327.645, 1454324.964...\n490039603021\nBlock Group 1\n801\n72872.0\n0\n\n\n2\n490039603011\nPOLYGON ((1458913.147 7790335.904, 1458913.726...\n490039603011\nBlock Group 1\n367\n99519.0\n0\n\n\n3\n490039603012\nPOLYGON ((1457125.723 7786638.014, 1457132.453...\n490039603012\nBlock Group 2\n505\n64970.0\n0\n\n\n4\n490039603013\nPOLYGON ((1457448.445 7792187.142, 1457460.447...\n490039603013\nBlock Group 3\n313\n71507.0\n0\n\n\n\n\n\n\n\n\n\nShow the code\n# Explore the processed block group median income data\ngdf_bg_with_income.explore(\n    column=\"Median Income\",\n    cmap=\"YlGnBu\",\n    legend=True,\n    tooltip=[\"GEOID\", \"NAMELSAD\", \"HH_TOTAL\", \"Median Income\", \"Flag\"]\n)\n\n\n\n\nStep 3: Prepare Census Blocks with Household Counts\nExtract the block group identifier from each census block’s GEOID. The first 12 characters of a block GEOID uniquely identify its parent block group, allowing us to link blocks to their income data.\n\n\nShow the code\n# Extract Block Group GEOID from Block GEOID (first 12 characters)\ngdf_block_hh['GEOID_BG'] = gdf_block_hh['GEOID'].str[:12]\n\n\nJoin the median income from block groups to individual census blocks. Each block inherits the income characteristics of its parent block group, as income data is not available at the block level.\n\n\n\n\n\n\nNote\n\n\n\nMedian income is transferred directly from block groups to blocks via a simple join on GEOID_BG. Each block inherits the median income of its parent block group.\n\n\n\n\nShow the code\n# Join median income from block groups to blocks\ngdf_block_hh_income = gdf_block_hh.merge(\n    df_bg_income_process[['GEOID_BG', 'Median Income']],\n    on='GEOID_BG',\n    how='left'\n)\n\nprint(f\"Blocks after join: {len(gdf_block_hh_income)}\")\nprint(f\"Blocks with median income: {gdf_block_hh_income['Median Income'].notna().sum()}\")\ngdf_block_hh_income.head()\n\n\nBlocks after join: 71207\nBlocks with median income: 71207\n\n\n\n\n\n\n\n\n\nGEOID\nHH20\ngeometry\nGEOID_BG\nMedian Income\n\n\n\n\n0\n490351143041007\n18\nPOLYGON ((1498087.949 7387694.013, 1498091.062...\n490351143041\n121328.0\n\n\n1\n490351130221042\n0\nPOLYGON ((1502208.892 7368252.412, 1502215.882...\n490351130221\n107939.0\n\n\n2\n490351128302015\n53\nPOLYGON ((1533128.823 7346280.041, 1533134.077...\n490351128302\n116250.0\n\n\n3\n490351139091031\n26\nPOLYGON ((1482871.57 7388668.635, 1482871.675 ...\n490351139091\n142650.0\n\n\n4\n490351128134002\n9\nPOLYGON ((1548140.763 7367459.474, 1548156.067...\n490351128134\n163500.0\n\n\n\n\n\n\n\n\n\nStep 4: Weighted Interpolation of 2023 Households to Blocks\nThe 2020 Decennial Census provides precise household counts, but we need 2023 estimates to match our income data year. This step uses spatial interpolation to distribute 2023 household totals from block groups to individual blocks, weighted by the 2020 block-level household distribution.\nUse the weighted interpolation function to proportionally allocate 2023 household totals from block groups to census blocks. The 2020 household counts serve as weights, assuming the relative distribution of households among blocks remains similar. This produces estimated 2023 household counts at the block level.\n\n\n\n\n\n\nNote\n\n\n\nThis step uses the interpolate_pw function to distribute 2023 ACS household totals from block groups to census blocks, weighted by 2020 household counts. We use extensive=True because household count is an extensive variable (sum).\n\n\n\n\nShow the code\n# Use weighted interpolation to transfer HH_TOTAL from BG to blocks\ngdf_blocks_hh23 = interpolate_pw(\n    from_gdf=gdf_bg_with_income[['GEOID_BG', 'HH_TOTAL', 'geometry']],\n    to_gdf=gdf_block_hh[['GEOID', 'geometry']],\n    weights_gdf=gdf_block_hh[['GEOID', 'HH20', 'geometry']],\n    to_id='GEOID',\n    extensive=True,  # Extensive variable - sum households\n    weight_column='HH20',\n    weight_placement='centroid',\n    crs=PROJECT_CRS\n)\n\n# Merge interpolated households with the blocks that have median income\ngdf_block_hh_income = gdf_block_hh_income.merge(\n    gdf_blocks_hh23[['GEOID', 'HH_TOTAL']].rename(columns={'HH_TOTAL': 'HH23'}),\n    on='GEOID',\n    how='left'\n)\n\n# Round and convert to integer\ngdf_block_hh_income['HH23'] = gdf_block_hh_income['HH23'].round(0).astype(int).fillna(0)\n\nprint(f\"Total 2023 HH in blocks: {gdf_block_hh_income['HH23'].sum():,.0f}\")\ngdf_block_hh_income.head()\n\n\nTotal 2023 HH in blocks: 1,094,760\n\n\n\n\n\n\n\n\n\nGEOID\nHH20\ngeometry\nGEOID_BG\nMedian Income\nHH23\n\n\n\n\n0\n490351143041007\n18\nPOLYGON ((1498087.949 7387694.013, 1498091.062...\n490351143041\n121328.0\n18\n\n\n1\n490351130221042\n0\nPOLYGON ((1502208.892 7368252.412, 1502215.882...\n490351130221\n107939.0\n0\n\n\n2\n490351128302015\n53\nPOLYGON ((1533128.823 7346280.041, 1533134.077...\n490351128302\n116250.0\n51\n\n\n3\n490351139091031\n26\nPOLYGON ((1482871.57 7388668.635, 1482871.675 ...\n490351139091\n142650.0\n33\n\n\n4\n490351128134002\n9\nPOLYGON ((1548140.763 7367459.474, 1548156.067...\n490351128134\n163500.0\n11\n\n\n\n\n\n\n\nAfter interpolation, the total estimated 2023 households across all blocks should closely match the ACS block group totals.\n\n\nStep 5: Polygon-to-Polygon Intersection between Blocks and TAZ\nCensus blocks and TAZ boundaries don’t align perfectly - blocks may span multiple TAZs and TAZs may contain partial blocks. A polygon overlay operation identifies all intersections and preserves the exact geometry of these overlaps.\nPerform a geometric intersection between census blocks and TAZ boundaries. This creates new polygons representing every unique combination of block and TAZ, along with the attributes from both datasets.\n\n\n\n\n\n\nNote\n\n\n\nInstead of using spatial join with centroids, we use polygon overlay to capture the actual intersection between census blocks and TAZs. This preserves spatial relationships when blocks cross TAZ boundaries.\n\n\n\n\nShow the code\n# Perform polygon-to-polygon intersection\ngdf_block_taz_intersection = gpd.overlay(\n    gdf_block_hh_income[['GEOID', 'GEOID_BG', 'HH20', 'HH23', 'Median Income', 'geometry']],\n    gdf_TAZ[['CO_TAZID', 'SUBAREAID', 'CO_FIPS', 'CO_NAME', 'geometry']],\n    how='intersection'\n)\n\nprint(f\"Total intersected polygons: {len(gdf_block_taz_intersection):,}\")\nprint(f\"Unique blocks in intersections: {gdf_block_taz_intersection['GEOID'].nunique():,}\")\nprint(f\"Unique TAZs in intersections: {gdf_block_taz_intersection['CO_TAZID'].nunique():,}\")\ngdf_block_taz_intersection.head()\n\n\nTotal intersected polygons: 142,200\nUnique blocks in intersections: 71,207\nUnique TAZs in intersections: 9,891\n\n\n\n\n\n\n\n\n\nGEOID\nGEOID_BG\nHH20\nHH23\nMedian Income\nCO_TAZID\nSUBAREAID\nCO_FIPS\nCO_NAME\ngeometry\n\n\n\n\n0\n490351143041007\n490351143041\n18\n18\n121328.0\n350830\n1\n35\nSALT LAKE\nPOLYGON ((1498091.062 7387704.197, 1498091.916...\n\n\n1\n490351130221042\n490351130221\n0\n0\n107939.0\n351072\n1\n35\nSALT LAKE\nMULTIPOLYGON (((1502215.882 7368260.024, 15022...\n\n\n2\n490351130221042\n490351130221\n0\n0\n107939.0\n351092\n1\n35\nSALT LAKE\nPOLYGON ((1502229.635 7368284.356, 1502230.673...\n\n\n3\n490351130221042\n490351130221\n0\n0\n107939.0\n351107\n1\n35\nSALT LAKE\nPOLYGON ((1502723.991 7368074.316, 1502503.507...\n\n\n4\n490351128302015\n490351128302\n53\n51\n116250.0\n351241\n1\n35\nSALT LAKE\nPOLYGON ((1536105.428 7347687.451, 1536104.998...\n\n\n\n\n\n\n\nThe intersection produces more records than either input dataset because blocks crossing TAZ boundaries create multiple intersection polygons.\n\n\nStep 6: Calculate Intersection Areas and Proportional Households\nWhen a census block spans multiple TAZs, we need to distribute its households proportionally based on the area of each intersection. This assumes households are evenly distributed within each block.\nCalculate the area of each intersection polygon and compare it to the original block area. The ratio determines what proportion of the block’s households fall within each TAZ.\n\n\nShow the code\n# Calculate area of each intersected polygon\ngdf_block_taz_intersection['intersection_area'] = gdf_block_taz_intersection.geometry.area\n\n# Calculate original block areas\nblock_areas = gdf_block_hh_income[['GEOID', 'geometry']].copy()\nblock_areas['block_area'] = block_areas.geometry.area\nblock_areas = block_areas[['GEOID', 'block_area']]\n\n# Merge block areas\ngdf_block_taz_intersection = gdf_block_taz_intersection.merge(\n    block_areas,\n    on='GEOID',\n    how='left'\n)\n\n# Calculate proportion of block area within each intersection\ngdf_block_taz_intersection['area_proportion'] = np.where(\n    gdf_block_taz_intersection['block_area'] &gt; 0,\n    gdf_block_taz_intersection['intersection_area'] / gdf_block_taz_intersection['block_area'],\n    0\n)\n\n\nDistribute households to each intersection based on the area proportion. For example, if 30% of a block’s area falls in one TAZ and 70% in another, we assign households proportionally.\n\n\nShow the code\n# Distribute households proportionally based on intersection area\ngdf_block_taz_intersection['HH23_intersect'] = (\n    gdf_block_taz_intersection['HH23'] * gdf_block_taz_intersection['area_proportion']\n).round(0)\n\nprint(f\"Total HH23 after proportional distribution: {gdf_block_taz_intersection['HH23_intersect'].sum():,.0f}\")\ngdf_block_taz_intersection.head()\n\n\nTotal HH23 after proportional distribution: 1,094,370\n\n\n\n\n\n\n\n\n\nGEOID\nGEOID_BG\nHH20\nHH23\nMedian Income\nCO_TAZID\nSUBAREAID\nCO_FIPS\nCO_NAME\ngeometry\nintersection_area\nblock_area\narea_proportion\nHH23_intersect\n\n\n\n\n0\n490351143041007\n490351143041\n18\n18\n121328.0\n350830\n1\n35\nSALT LAKE\nPOLYGON ((1498091.062 7387704.197, 1498091.916...\n319529.133518\n3.195291e+05\n1.000000\n18.0\n\n\n1\n490351130221042\n490351130221\n0\n0\n107939.0\n351072\n1\n35\nSALT LAKE\nMULTIPOLYGON (((1502215.882 7368260.024, 15022...\n152.575072\n6.534741e+04\n0.002335\n0.0\n\n\n2\n490351130221042\n490351130221\n0\n0\n107939.0\n351092\n1\n35\nSALT LAKE\nPOLYGON ((1502229.635 7368284.356, 1502230.673...\n64495.479047\n6.534741e+04\n0.986963\n0.0\n\n\n3\n490351130221042\n490351130221\n0\n0\n107939.0\n351107\n1\n35\nSALT LAKE\nPOLYGON ((1502723.991 7368074.316, 1502503.507...\n699.358448\n6.534741e+04\n0.010702\n0.0\n\n\n4\n490351128302015\n490351128302\n53\n51\n116250.0\n351241\n1\n35\nSALT LAKE\nPOLYGON ((1536105.428 7347687.451, 1536104.998...\n2.751998\n1.193632e+06\n0.000002\n0.0\n\n\n\n\n\n\n\nThe total households after proportional distribution should match the sum before distribution, confirming that no households were lost or duplicated in the process.\n\n\nStep 7: Calculate TAZ Median Income using Weighted Mean\nRather than simply averaging block-level incomes, we need to calculate a household-weighted mean for each TAZ. This ensures that blocks with more households have more influence on the TAZ median.\nFor each intersection polygon, calculate the product of households and median income. This weighted value will be used to compute the household-weighted mean income for each TAZ.\n\n\nShow the code\n# Calculate weighted income (HH * Median Income) for each intersected polygon\ngdf_block_taz_intersection['HH_MedInc_Product'] = (\n    gdf_block_taz_intersection['HH23_intersect'] * gdf_block_taz_intersection['Median Income']\n)\n\n\nGroup all intersections by TAZ and calculate the weighted mean income. Divide the sum of weighted income products by the sum of households to get the final TAZ median. For TAZs with zero households, use the simple median of block incomes as a fallback.\n\n\nShow the code\n# Group by TAZ and calculate weighted mean\ndf_taz_aggregated = gdf_block_taz_intersection.groupby('CO_TAZID').agg({\n    'SUBAREAID': 'first',\n    'CO_FIPS': 'first',\n    'CO_NAME': 'first',\n    'HH23_intersect': 'sum',\n    'HH_MedInc_Product': 'sum',\n    'Median Income': 'median'  # For fallback if HH23 = 0\n}).reset_index()\n\n# Rename column for clarity\ndf_taz_aggregated = df_taz_aggregated.rename(columns={'HH23_intersect': 'HH23'})\n\n# Calculate TAZ median income using weighted mean\ndf_taz_aggregated['MedInc'] = np.where(\n    df_taz_aggregated['HH23'].notna() & (df_taz_aggregated['HH23'] &gt; 0),\n    (df_taz_aggregated['HH_MedInc_Product'] / df_taz_aggregated['HH23']).round(0),\n    df_taz_aggregated['Median Income'].round(0)\n)\n\nprint(f\"\\nTotal TAZs: {len(df_taz_aggregated)}\")\nprint(f\"TAZs with median income: {df_taz_aggregated['MedInc'].notna().sum()}\")\nprint(f\"TAZs with NULL median income: {df_taz_aggregated['MedInc'].isna().sum()}\")\ndf_taz_aggregated.head(10)\n\n\n\nTotal TAZs: 9891\nTAZs with median income: 9891\nTAZs with NULL median income: 0\n\n\n\n\n\n\n\n\n\nCO_TAZID\nSUBAREAID\nCO_FIPS\nCO_NAME\nHH23\nHH_MedInc_Product\nMedian Income\nMedInc\n\n\n\n\n0\n1001\n0\n1\nBEAVER\n0.0\n0.0\n140069.0\n140069.0\n\n\n1\n1002\n0\n1\nBEAVER\n22.0\n1666874.0\n140069.0\n75767.0\n\n\n2\n1003\n0\n1\nBEAVER\n33.0\n2500311.0\n75767.0\n75767.0\n\n\n3\n1004\n0\n1\nBEAVER\n4.0\n303068.0\n75767.0\n75767.0\n\n\n4\n1005\n0\n1\nBEAVER\n109.0\n7281200.0\n66800.0\n66800.0\n\n\n5\n1006\n0\n1\nBEAVER\n67.0\n5076389.0\n75767.0\n75767.0\n\n\n6\n1007\n0\n1\nBEAVER\n288.0\n19238400.0\n66800.0\n66800.0\n\n\n7\n1008\n0\n1\nBEAVER\n172.0\n11920016.0\n75767.0\n69302.0\n\n\n8\n1009\n0\n1\nBEAVER\n102.0\n7728234.0\n75767.0\n75767.0\n\n\n9\n1010\n0\n1\nBEAVER\n113.0\n7647037.0\n66800.0\n67673.0\n\n\n\n\n\n\n\n\n\nStep 8: Handle Remaining NULL Values (County Fallback)\nDespite our multi-step process, some TAZs may still lack income estimates, typically in unpopulated areas or areas with data quality issues. County-level medians provide a reasonable final fallback.\nCheck for any TAZs with missing median income values and fill them with their county’s median. This ensures every TAZ has an income estimate, even if it’s less precise than block-level data.\n\n\nShow the code\n# Check if any TAZs still have NULL values\nif df_taz_aggregated['MedInc'].isna().any():\n    print(f\"Applying county median fallback for {df_taz_aggregated['MedInc'].isna().sum()} TAZs...\")\n\n    # Get county medians\n    county_medians = gdf_county_income[['COUNTYFP', 'HH_MED_INC']].copy()\n    county_medians['CO_FIPS'] = county_medians['COUNTYFP'].astype(int)\n    county_medians = county_medians.rename(columns={'HH_MED_INC': 'County_MedInc'})\n\n    # Merge with TAZ data\n    df_taz_aggregated = df_taz_aggregated.merge(\n        county_medians[['CO_FIPS', 'County_MedInc']],\n        on='CO_FIPS',\n        how='left'\n    )\n\n    # Fill NULL values with county median\n    df_taz_aggregated['MedInc'] = df_taz_aggregated['MedInc'].fillna(\n        df_taz_aggregated['County_MedInc']\n    )\n\n    print(f\"After county fallback - TAZs with NULL: {df_taz_aggregated['MedInc'].isna().sum()}\")\nelse:\n    print(\"No NULL values found - all TAZs have median income!\")\n\n\nNo NULL values found - all TAZs have median income!"
  },
  {
    "objectID": "index.html#results-visualizations",
    "href": "index.html#results-visualizations",
    "title": "TAZ & Regional Median Income",
    "section": "7 Results & Visualizations",
    "text": "7 Results & Visualizations\n\nTAZ Median Income Summary\nThe final dataset contains median income estimates for all 9891 TAZs in Utah, along with estimated 2023 household counts. These values can be used directly in transportation models and planning analyses.\n\n\nShow the code\n# Create final TAZ output\ndf_taz_income = df_taz_aggregated[[\n    'CO_TAZID',\n    'SUBAREAID',\n    'CO_FIPS',\n    'CO_NAME',\n    'MedInc',\n    'HH23'\n]].copy()\n\n# Sort by CO_TAZID\ndf_taz_income = df_taz_income.sort_values('CO_TAZID').reset_index(drop=True)\n\nprint(\"=\"*80)\nprint(\"FINAL TAZ MEDIAN INCOME RESULTS\")\nprint(\"=\"*80)\ndf_taz_income\n\n\n================================================================================\nFINAL TAZ MEDIAN INCOME RESULTS\n================================================================================\n\n\n\n\n\n\n\n\n\nCO_TAZID\nSUBAREAID\nCO_FIPS\nCO_NAME\nMedInc\nHH23\n\n\n\n\n0\n1001\n0\n1\nBEAVER\n140069.0\n0.0\n\n\n1\n1002\n0\n1\nBEAVER\n75767.0\n22.0\n\n\n2\n1003\n0\n1\nBEAVER\n75767.0\n33.0\n\n\n3\n1004\n0\n1\nBEAVER\n75767.0\n4.0\n\n\n4\n1005\n0\n1\nBEAVER\n66800.0\n109.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n9886\n570424\n1\n57\nWEBER\n138832.0\n7.0\n\n\n9887\n570425\n1\n57\nWEBER\n126131.0\n68.0\n\n\n9888\n570426\n1\n57\nWEBER\n184236.0\n28.0\n\n\n9889\n570427\n1\n57\nWEBER\n114110.0\n57.0\n\n\n9890\n570428\n1\n57\nWEBER\n153261.0\n156.0\n\n\n\n\n9891 rows × 6 columns\n\n\n\nTAZs with zero households typically represent unpopulated areas, employment centers, or future development zones. Their assigned income values (from county fallbacks) should be used cautiously.\n\n\nShow the code\n# Display TAZs with no households\nno_hh_tazs = df_taz_income[df_taz_income['HH23'] == 0]\nif len(no_hh_tazs) &gt; 0:\n    print(f\"\\nTAZs with no households: {len(no_hh_tazs)}\")\n    print(no_hh_tazs[['CO_TAZID', 'CO_NAME', 'MedInc', 'HH23']].head(10))\n\n\n\nTAZs with no households: 1652\n    CO_TAZID CO_NAME    MedInc  HH23\n0       1001  BEAVER  140069.0   0.0\n15      1016  BEAVER  140069.0   0.0\n16      1017  BEAVER  140069.0   0.0\n17      1018  BEAVER  140069.0   0.0\n27      1028  BEAVER  140069.0   0.0\n29      1030  BEAVER  140069.0   0.0\n40      1041  BEAVER  140069.0   0.0\n41      1042  BEAVER  140069.0   0.0\n44      1045  BEAVER  140069.0   0.0\n45      1046  BEAVER  140069.0   0.0\n\n\n\n\nInteractive Map: TAZ Median Income\nVisualize the spatial distribution of median income across Utah’s TAZs. The map reveals clear geographic patterns, with higher incomes concentrated in suburban areas and lower incomes in urban cores and rural regions.\n\n\nShow the code\n# Visualize the distribution of TAZ median incomes\ngdf_TAZ[['CO_TAZID', 'geometry']].merge(\n    df_taz_income[['CO_TAZID', 'SUBAREAID', 'CO_FIPS', 'CO_NAME', 'MedInc', 'HH23']],\n    on='CO_TAZID',\n    how='left'\n).explore(\n    column=\"MedInc\",\n    cmap=\"YlGnBu\",\n    legend=True,\n    tooltip=[\"CO_TAZID\", \"CO_NAME\", \"MedInc\", \"HH23\"],\n    popup=True\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nRegional Median Income Summary\nAggregate TAZ-level data to calculate household-weighted median income for each planning region. These regional values are particularly useful for comparative analysis and regional planning efforts.\n\n\nShow the code\n# Calculate regional weighted medians\ndf_regional_aggregated = df_taz_aggregated.groupby('SUBAREAID').agg({\n    'HH23': 'sum',\n    'HH_MedInc_Product': 'sum'\n}).reset_index()\n\ndf_regional_aggregated['MedInc'] = np.where(\n    df_regional_aggregated['HH23'] &gt; 0,\n    (df_regional_aggregated['HH_MedInc_Product'] / df_regional_aggregated['HH23']).round(0),\n    np.nan\n).astype(int)\n\ndf_regional_aggregated['Model Space'] = df_regional_aggregated['SUBAREAID'].map(subareaid_lookup)\n\ndf_regional_income = df_regional_aggregated[[\n    'SUBAREAID',\n    'Model Space',\n    'MedInc',\n    'HH23'\n]].copy().sort_values('SUBAREAID').reset_index(drop=True)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REGIONAL MEDIAN INCOME SUMMARY\")\nprint(\"=\"*80)\ndf_regional_income\n\n\n\n================================================================================\nREGIONAL MEDIAN INCOME SUMMARY\n================================================================================\n\n\n\n\n\n\n\n\n\nSUBAREAID\nModel Space\nMedInc\nHH23\n\n\n\n\n0\n0\nUDOT\n80317\n111853.0\n\n\n1\n1\nWasatch Front\n101132\n826549.0\n\n\n2\n2\nCache\n82460\n43043.0\n\n\n3\n3\nDixie\n81171\n66865.0\n\n\n4\n4\nSummit-Wasatch\n135154\n26428.0\n\n\n5\n5\nIron\n68317\n19632.0\n\n\n\n\n\n\n\nThe Summit-Wasatch region, which includes resort communities, shows the highest median income, followed by Wasatch Front region (containing the largest metropolitan areas). Rural regions generally have lower median incomes, reflecting different economic structures and costs of living.\n\n\nResult Validation\nTo ensure our methodology produces reasonable results, we compare our 2023 estimates with previously calculated 2019 values. We expect median incomes to increase due to both inflation and real wage growth.\nLoad the 2019 regional median income data from the previous analysis for comparison.\n\n\nShow the code\n# Compare county median income with 2019 calculations\ndf_regional_income_2019 = pd.read_excel(\n  r\"_archive/_Source - TAZ & Regional Median Income - 2022-03-17.xlsb\",\n  sheet_name=\"Export_MedInc\",\n  usecols=\"H:J\",\n  skiprows=4,\n  nrows=6,\n  engine=\"pyxlsb\"\n)\n\ndf_regional_income_2019[['MedInc_2019']] = df_regional_income_2019[['MedInc']].astype(int)\n\n\nCalculate the absolute and percentage change between 2019 and 2023. The changes should be consistent with inflation rates and economic growth over this period. Typical increases of 20-30% over four years align with the combination of ~20% cumulative inflation plus modest real wage growth during this period.\n\n\nShow the code\n# Merge 2019 and 2023 data\ndf_regional_comparison = df_regional_income[['SUBAREAID', 'Model Space', 'MedInc']].rename(columns={\n    'MedInc': 'MedInc_2023'\n}).merge(\n    df_regional_income_2019[['SUBAREAID', 'MedInc_2019']],\n    on='SUBAREAID',\n    how='left'\n)\n\n# Calculate absolute and percent change\ndf_regional_comparison['Absolute_Change'] = (\n    df_regional_comparison['MedInc_2023'] - df_regional_comparison['MedInc_2019']\n)\n\ndf_regional_comparison['Percent_Change'] = (\n    (df_regional_comparison['MedInc_2023'] - df_regional_comparison['MedInc_2019'])\n    / df_regional_comparison['MedInc_2019'] * 100\n).round(2)\n\n\n\n# Display comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"REGIONAL MEDIAN INCOME COMPARISON: 2019 vs 2023\")\nprint(\"=\"*80)\ndf_regional_comparison\n\n\n\n================================================================================\nREGIONAL MEDIAN INCOME COMPARISON: 2019 vs 2023\n================================================================================\n\n\n\n\n\n\n\n\n\nSUBAREAID\nModel Space\nMedInc_2023\nMedInc_2019\nAbsolute_Change\nPercent_Change\n\n\n\n\n0\n0\nUDOT\n80317\n61912\n18405\n29.73\n\n\n1\n1\nWasatch Front\n101132\n74946\n26186\n34.94\n\n\n2\n2\nCache\n82460\n59039\n23421\n39.67\n\n\n3\n3\nDixie\n81171\n59836\n21335\n35.66\n\n\n4\n4\nSummit-Wasatch\n135154\n96562\n38592\n39.97\n\n\n5\n5\nIron\n68317\n51808\n16509\n31.87\n\n\n\n\n\n\n\nThe comparison shows consistent increases across all regions, with percentage changes ranging from approximately 20-30%. This aligns with economic trends during 2019-2023, including significant inflation, particularly in 2021-2022. The Wasatch Front shows slightly lower percentage growth, which may reflect its already-high base, while smaller regions show more variation due to sample size effects.\n\n\nChart\n\nCounty Median Income Distribution\nThis scatter plot ranks Utah’s 29 counties by median household income and visualizes the relationship between rank and income level. Bubble size represents total households in each county, highlighting the population concentration in metropolitan areas.\n\n\nShow the code\n# Create plotting dataframe\ndf_plot = gdf_county_income[['NAMELSAD', 'HH_MED_INC', 'HH_TOTAL']].copy()\n\n# Rank counties by median household income\ndf_plot[\"Rank\"] = df_plot[\"HH_MED_INC\"].rank(\n    method=\"min\",\n    ascending=False\n).astype(int)\n\n# Create Region column based on MPO counties\nmpo_counties = ['Summit County', 'Wasatch County', 'Davis County', 'Salt Lake County',\n                'Utah County', 'Weber County', 'Washington County', 'Cache County', 'Iron County']\n\ndf_plot['Region'] = df_plot['NAMELSAD'].apply(lambda x: 'MPO' if x in mpo_counties else 'Non-MPO')\n\n# Define color mapping\ncolor_map = {'MPO': '#843C0C', 'Non-MPO': '#4472C4'}\n\n\n\n\nShow the code\n# Set style\nsns.set_style(\"whitegrid\")\n\n# Create figure\n# plt.figure(figsize=(10, 8))\n\n# Create scatter plot\nsns.scatterplot(\n    data=df_plot,\n    x='Rank',\n    y='HH_MED_INC',\n    size='HH_TOTAL',\n    sizes=(50, 7500),\n    hue='Region',\n    palette=color_map,\n    alpha=0.3,\n    edgecolor=df_plot['Region'].map(color_map),\n    linewidth=2.5,\n    legend=False\n)\n\n# Add smooth trend line\nsns.regplot(\n    data=df_plot,\n    x='Rank',\n    y='HH_MED_INC',\n    scatter=False,\n    order=3,\n    ci=None,\n    color='red',\n    line_kws={'linestyle':':', 'dashes':(1, 1), 'linewidth':2, 'alpha':0.8}\n)\n\n# Create list to store text objects\ntexts = []\n\n# Add labels for each county\nfor idx, row in df_plot.iterrows():\n    county_name = row['NAMELSAD'].replace(' County', '').upper()\n    income_label = f\"${row['HH_MED_INC']:,.0f}\"\n    label = f\"{county_name}\\n{income_label}\"\n    label_color = color_map[row['Region']]\n\n    text = plt.annotate(\n        label,\n        xy=(row['Rank'], row['HH_MED_INC']),\n        xytext=(row['Rank'], row['HH_MED_INC']),\n        textcoords='data',\n        ha='center',\n        va='center',\n        fontsize=7,\n        fontweight='bold',\n        color=label_color\n    )\n    texts.append(text)\n\n# Adjust text positions to avoid overlap\nadjust_text(\n    texts,\n    arrowprops=dict(arrowstyle='-', color='gray', lw=0.5, alpha=0.6, shrinkA=10, shrinkB=10),\n    lim=500\n)\n\n# Add horizontal line at state median\nstate_avg = gdf_ut_income['HH_MED_INC'].values[0]\nplt.axhline(y=state_avg, color='#C00000', linestyle='--', linewidth=2, alpha=0.4)\n\n# Add text annotation for state average\nplt.text(29.5, state_avg + 1500, f'Utah State Median: ${state_avg:,.0f}',\n         fontsize=10, color='#C00000', ha='right', va='bottom', fontweight='bold')\n\n# Set axis limits and ticks\nplt.xlim(0, 30)\nplt.ylim(0, 150000)\nplt.xticks(range(0, 31, 5))\n\n# Format y-axis\nplt.gca().yaxis.set_major_locator(ticker.MultipleLocator(20000))\nplt.gca().yaxis.set_minor_locator(ticker.MultipleLocator(10000))\nplt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x:,.0f}'))\n\n# Customize grid\nplt.grid(True, which='major', axis='y', alpha=0.4, linewidth=0.8, color='gray')\nplt.grid(True, which='minor', axis='y', alpha=0.2, linewidth=0.5, color='gray')\n\n# Set labels and title\nplt.xlabel('Rank', fontsize=12, fontweight='bold', color='#595959')\nplt.ylabel('Median Household Income', fontsize=12, fontweight='bold', color='#595959')\nplt.title('Median Income in 2023 Dollars', fontsize=16, fontweight='bold', pad=20, color='#595959')\nplt.gca().text(0.5, 1.02, 'Source: 2019-2023 American Community Survey',\n               transform=plt.gca().transAxes, ha='center', fontsize=10.5,\n               color='#595959', style='italic')\n\nplt.tight_layout()\nplt.show()\n\n\nLooks like you are using a tranform that doesn't support FancyArrowPatch, using ax.annotate instead. The arrows might strike through texts. Increasing shrinkA in arrowprops might help.\n\n\n\n\n\n\n\n\n\nThe chart distinguishes between MPO (Metropolitan Planning Organization) and non-MPO counties, revealing that most high-income counties are part of MPO regions. The curved trend line shows that income differences between top-ranked counties are larger than differences among lower-ranked counties. The state median provides context for interpreting individual county values.\n\n\nCounty vs Block Group Median Comparison\nThis validation chart compares county-level median income (reported directly by ACS) with the median of all block group medians within each county. If our block group data is reliable, these two values should be highly correlated.\n\n\nShow the code\n# Calculate median of block group median incomes for each county\nbg_median_by_county = gdf_bg_income[\n    gdf_bg_income['HH_MED_INC'].notna()\n].groupby('COUNTYFP')['HH_MED_INC'].median().reset_index()\nbg_median_by_county.columns = ['COUNTYFP', 'BG_MedianOfMedians']\n\n# Merge with county data\nplot_data = gdf_county_income[['COUNTYFP', 'NAMELSAD', 'HH_MED_INC']].merge(\n    bg_median_by_county,\n    on='COUNTYFP',\n    how='left'\n)\n\n# Filter out missing values\nplot_data = plot_data[\n    (plot_data['HH_MED_INC'].notna()) &\n    (plot_data['BG_MedianOfMedians'].notna())\n].copy()\n\n# Set style\nsns.set_style(\"whitegrid\")\n\n# Create figure\n# plt.figure(figsize=(10, 8))\n\n# Create scatter plot\nsns.scatterplot(\n    data=plot_data,\n    x='BG_MedianOfMedians',\n    y='HH_MED_INC',\n    alpha=0.6,\n    s=150,\n    color='#4472C4',\n    edgecolor='#4472C4',\n    linewidth=2,\n    legend=False\n)\n\n# Add regression line\nsns.regplot(\n    data=plot_data,\n    x='BG_MedianOfMedians',\n    y='HH_MED_INC',\n    scatter=False,\n    color='#C00000',\n    line_kws={\n        'linestyle': '--',\n        'linewidth': 2,\n        'alpha': 0.7,\n        'label': 'Trend line'\n    },\n    ci=None\n)\n\n# Add diagonal reference line (y=x)\nmin_val = min(plot_data['BG_MedianOfMedians'].min(), plot_data['HH_MED_INC'].min())\nmax_val = max(plot_data['BG_MedianOfMedians'].max(), plot_data['HH_MED_INC'].max())\nplt.plot([min_val, max_val], [min_val, max_val],\n         color='gray',\n         linestyle=':',\n         linewidth=1.5,\n         alpha=0.5,\n         label='y=x (perfect match)')\n\n# Add county labels\nfor idx, row in plot_data.iterrows():\n    county_name = row['NAMELSAD'].replace(' County', '')\n    plt.annotate(\n        county_name,\n        xy=(row['BG_MedianOfMedians'], row['HH_MED_INC']),\n        xytext=(5, 5),\n        textcoords='offset points',\n        fontsize=8,\n        alpha=0.7\n    )\n\n# Format axes\nplt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nplt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n\n# Labels and title\nplt.xlabel('Median of Block Group Median Incomes', fontsize=12, fontweight='bold', color='#595959')\nplt.ylabel('County Median Income', fontsize=12, fontweight='bold', color='#595959')\nplt.title('County Median Income vs Median of Block Group Median Incomes',\n          fontsize=14, fontweight='bold', pad=15, color='#595959')\n\n# Add legend\nplt.legend(loc='lower right', fontsize=9)\n\n# Add correlation coefficient\ncorr = plot_data['BG_MedianOfMedians'].corr(plot_data['HH_MED_INC'])\nplt.text(0.02, 0.98, f'Correlation: {corr:.3f}\\nn={len(plot_data)} counties',\n         transform=plt.gca().transAxes,\n         fontsize=10,\n         verticalalignment='top',\n         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nA strong correlation (close to 1.0) indicates that block group data accurately reflects county-level patterns. Points falling near the diagonal reference line (y=x) show counties where the two measures align closely. Deviations suggest either data quality issues at the block group level or genuine within-county variation in how income is distributed.\n\n\n2019 vs 2023 Block Group Income Distribution\n\n\nShow the code\n# Fetch block group boundaries from TIGER/Line shapefiles\ngdf_bg_bound19 = block_groups(\n  state=STATE_FIPS,\n  year=2019,\n  cache=True\n).to_crs(PROJECT_CRS)\n\n# Fetch Income data from ACS 5-year estimates for block groups in Utah\ndf_bg_income19 = get_census(\n  dataset=\"acs/acs5\",\n  year=2019,\n  variables=list(acs_variables.keys()),\n  params={\n      # \"key\": f\"{os.getenv('CENSUS_API_KEY')}\", # FIXME: This causes error\n      \"for\": f\"block group:*\",\n      \"in\": f\"state:{STATE_FIPS} county:*\"\n    },\n    return_geoid=True,\n    guess_dtypes=True\n)\n\n# Join ACS data to block group boundaries and transform CRS\ngdf_bg_income19 = gdf_bg_bound19[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(\n    df_bg_income19, on = \"GEOID\"\n).to_crs(PROJECT_CRS).rename(columns=acs_variables)\n\ngdf_bg_income19.head(10)\n\n\n\n\n\n\n\n\n\nGEOID\nCOUNTYFP\nNAMELSAD\ngeometry\nHH_MED_INC\nHH_MED_INC_MOE\nHH_TOTAL\nHH_LT_10K\nHH_10_15K\nHH_15_20K\n...\nHH_35_40K\nHH_40_45K\nHH_45_50K\nHH_50_60K\nHH_60_75K\nHH_75_100K\nHH_100_125K\nHH_125_150K\nHH_150_200K\nHH_GT_200K\n\n\n\n\n0\n490419753001\n041\nBlock Group 1\nPOLYGON ((1432909.072 6746155.981, 1433291.627...\n57313.0\n13917.0\n354\n0\n12\n0\n...\n6\n42\n17\n40\n48\n61\n25\n9\n21\n10\n\n\n1\n490419755002\n041\nBlock Group 2\nPOLYGON ((1347790.308 6631764.458, 1347816.171...\n46875.0\n16339.0\n298\n19\n8\n11\n...\n26\n42\n9\n16\n53\n30\n11\n3\n12\n18\n\n\n2\n490050007021\n005\nBlock Group 1\nPOLYGON ((1554722.973 7806743.364, 1554725.701...\n26071.0\n7182.0\n64\n0\n12\n0\n...\n0\n0\n6\n5\n0\n7\n0\n0\n0\n0\n\n\n3\n490490031061\n049\nBlock Group 1\nPOLYGON ((1609085.341 7220529.336, 1609099.065...\n68269.0\n12632.0\n306\n5\n5\n12\n...\n5\n5\n4\n54\n64\n69\n43\n6\n19\n0\n\n\n4\n490050006004\n005\nBlock Group 4\nPOLYGON ((1551900.381 7807124.928, 1551903.919...\n43865.0\n8397.0\n399\n26\n48\n13\n...\n0\n87\n74\n0\n28\n58\n0\n0\n0\n0\n\n\n5\n490351126171\n035\nBlock Group 1\nPOLYGON ((1546166.25 7374225.665, 1546183.231 ...\n136979.0\n19049.0\n321\n0\n0\n0\n...\n0\n0\n0\n35\n14\n26\n24\n77\n69\n69\n\n\n6\n490351110023\n035\nBlock Group 3\nPOLYGON ((1552325.981 7399744.208, 1552354.701...\n158750.0\n46880.0\n516\n0\n0\n0\n...\n0\n0\n0\n56\n44\n68\n55\n16\n89\n188\n\n\n7\n490351128051\n035\nBlock Group 1\nPOLYGON ((1539605.931 7367193.52, 1539647.07 7...\n114808.0\n22771.0\n506\n0\n0\n0\n...\n20\n28\n0\n18\n32\n83\n97\n65\n107\n56\n\n\n8\n490351119043\n035\nBlock Group 3\nPOLYGON ((1539061.081 7413087.421, 1539165.695...\n42528.0\n10022.0\n651\n86\n25\n40\n...\n25\n62\n42\n60\n56\n70\n14\n32\n0\n7\n\n\n9\n490351128233\n035\nBlock Group 3\nPOLYGON ((1531368.376 7363782.971, 1531422.446...\n96389.0\n11230.0\n904\n0\n29\n0\n...\n0\n34\n34\n84\n95\n182\n85\n105\n131\n73\n\n\n\n\n10 rows × 23 columns\n\n\n\nCompare the distribution of block group median incomes between the 2015-2019 and 2019-2023 ACS periods. The rightward shift of the 2023 distribution reflects both inflation and real income growth.\n\n\nShow the code\n# Prepare data: Combine 2019 and 2023 block group income data\ndf_bg_comparison = pd.DataFrame({\n    'Median Income': pd.concat([\n        gdf_bg_income19['HH_MED_INC'].dropna(),\n        gdf_bg_income['HH_MED_INC'].dropna()\n    ]),\n    'Year': ['2019'] * gdf_bg_income19['HH_MED_INC'].notna().sum() +\n            ['2023'] * gdf_bg_income['HH_MED_INC'].notna().sum()\n})\n\n\n\n\nShow the code\n# Plot: Overlaid histograms with 20 bins\nsns.set_style(\"whitegrid\")\n# plt.figure(figsize=(10, 8))\n\nsns.histplot(\n    data=df_bg_comparison,\n    x='Median Income',\n    hue='Year',\n    bins=20,\n    palette={'2019': '#843C0C', '2023': '#4472C4'},\n    alpha=0.6,\n    stat='count',\n    element='bars',\n    kde=False\n)\n\nplt.title('Block Group Median Income Distribution\\n2019 vs 2023',\n          fontweight='bold', fontsize=14, pad=15)\nplt.xlabel('Median Household Income ($)', fontweight='bold', fontsize=11)\nplt.ylabel('Number of Block Groups', fontweight='bold', fontsize=11)\nplt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe overlaid histograms reveal how the entire income distribution shifted upward. The 2023 distribution shows fewer block groups in the lowest income brackets and more in middle and upper brackets, consistent with the inflationary period and wage increases during this time.\n\n\n2019 vs 2023 Regional Median Income Comparison\nThe grouped bar chart directly compares regional median incomes between 2019 and 2023, making the magnitude of change clearly visible across all planning regions.\n\n\nShow the code\n# Prepare long format data\ndf_long = df_regional_comparison.melt(\n    id_vars=['Model Space'],\n    value_vars=['MedInc_2019', 'MedInc_2023'],\n    var_name='Year',\n    value_name='Median Income'\n)\ndf_long['Year'] = df_long['Year'].str.replace('MedInc_', '')\n\n\n\n\nShow the code\n# Plot 1: Grouped bar chart\nsns.set_style(\"whitegrid\")\n# plt.figure(figsize=(10, 8))\nax = sns.barplot(data=df_long, x='Model Space', y='Median Income', hue='Year',\n                 palette={'2019': '#843C0C', '2023': '#4472C4'})\nplt.title('Regional Median Income\\n2019 vs 2023', fontweight='bold', pad=15)\nplt.xticks(rotation=45, ha='right')\nplt.ylabel('Median Household Income ($)', fontweight='bold')\nplt.xlabel('Model Space', fontweight='bold')\nax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\nax.bar_label(ax.containers[0], labels=[f'${x:,.0f}' for x in ax.containers[0].datavalues], fontsize=8)\nax.bar_label(ax.containers[1], labels=[f'${x:,.0f}' for x in ax.containers[1].datavalues], fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Plot 2: Percent change\ncolors = ['green' if x &gt;= 0 else 'red' for x in df_regional_comparison['Percent_Change']]\nsns.set_style(\"whitegrid\")\n# plt.figure(figsize=(10, 8))\nsns.barplot(data=df_regional_comparison, y='Model Space', x='Percent_Change',\n            hue='Model Space', palette=colors, legend=False)\nplt.title('Percent Change in Median Income\\n(2019 to 2023)', fontweight='bold', pad=15)\nplt.xlabel('Percent Change (%)', fontweight='bold')\nplt.ylabel('')\nplt.axvline(x=0, color='black', linewidth=0.8)\n\n# Add percent labels\nfor i, val in enumerate(df_regional_comparison['Percent_Change']):\n    plt.text(val, i, f' {val:.1f}%', ha='left' if val &gt;= 0 else 'right',\n            va='center', fontweight='bold', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAll regions show substantial increases, with the absolute dollar increases being largest in already-high-income regions like Wasatch Front and Summit-Wasatch. The percentage change chart normalizes these increases, showing that rural regions experienced similar or sometimes higher percentage growth despite lower baseline incomes."
  },
  {
    "objectID": "index.html#export-results",
    "href": "index.html#export-results",
    "title": "TAZ & Regional Median Income",
    "section": "8 Export Results",
    "text": "8 Export Results\nSave the final TAZ and regional median income datasets as CSV files for use in the travel demand model and other planning applications. These files can be joined to other datasets using TAZ IDs or region IDs.\n\n\nShow the code\n# Create output directory if it doesn't exist\noutput_dir = Path(\"_output\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Export to CSV\ndf_taz_income.to_csv(\n    output_dir / \"taz_median_income.csv\",\n    index=False\n)\n\n# Export to CSV\ndf_regional_income.to_csv(\n    output_dir / \"regional_median_income.csv\",\n    index=False\n)\n\n\nThe exported files provide ready-to-use income estimates that reflect the most recent available data (2019-2023 ACS) and have been carefully processed to handle data quality issues while preserving spatial accuracy.\n\n\n\n\n\n\nTipDownload the output files:\n\n\n\ntaz_median_income.csv | regional_median_income.csv"
  }
]