---
title: TAZ & Regional Median Income
subtitle: Calculating Median Income for Transportation Analysis Zones (TAZ)
description: This notebook replicates the analysis from the '_Source - TAZ & Regional Median Income - 2022-03-17.xlsb' using the updated household and household income data from the 2019-2023 American Community Survey.
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-10-08"

execute:
   eval: true
jupyter: python3

format:
  html:
    theme:
        light: flatly
        dark: darkly
    respect-user-color-scheme: true

    fig-width: 8.4
    fig-height: 5.44
    fig-responsive: true

    toc: true
    number-sections: true
    number-depth: 2
    html-math-method: katex
    code-link: true
    code-tools: true
    code-fold: true
    code-summary: "Show the code"
    # fig-width: 8
    # fig-height: 5
    # out-width: "100%"
    # fig-align: center
    resources:
        - "_output/*.csv"

title-block-banner: true
---

## Introduction

This analysis calculates median household income for Transportation Analysis Zones (TAZ) across Utah using data from the 2019-2023 American Community Survey (ACS). The methodology employs spatial interpolation techniques to transfer income data from Census geographies (block groups and blocks) to TAZ boundaries, accounting for spatial overlaps and using household counts as weights.

The process involves several key steps: obtaining income data at the block group level, distributing this data to census blocks, and then aggregating to TAZ boundaries using area-weighted interpolation. This ensures that income estimates accurately reflect the spatial distribution of households within each TAZ.

## Environment Setup

### Install Required Packages

Install the necessary Python packages for data processing, spatial analysis, and visualization. This includes pandas for data manipulation, geopandas for spatial operations, pygris for accessing Census TIGER/Line shapefiles, and various visualization libraries.

``` python
!conda install -c conda-forge numpy pandas geopandas matplotlib seaborn python-dotenv openpyxl
!pip install pygris adjustText
```

### Load Libraries

Import all required libraries for the analysis. The pygris library enables direct access to Census Bureau geographic data and ACS estimates through their API.

```{python}
# For Analysis
import numpy as np
import pandas as pd
import geopandas as gpd
import warnings

# For Visualization
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
import seaborn as sns
from adjustText import adjust_text

# Census data query libraries & modules
from pygris import blocks, block_groups, counties, states
from pygris.helpers import validate_state, validate_county
from pygris.data import get_census

# misc
import datetime
import os
from pathlib import Path
import requests

from dotenv import load_dotenv
load_dotenv()
```

### Environment Variables

Set the coordinate reference system for the analysis to NAD83 / UTM zone 12N (EPSG:3566), which is appropriate for Utah. Also validate the state FIPS code for Utah.

```{python}
PROJECT_CRS = "EPSG:3566"  # NAD83 / UTM zone 12N
STATE_FIPS = validate_state("UT")
```

::: {.callout-tip}
**Need a Census API key?** Get one for free at [census.gov/developers](https://api.census.gov/data/key_signup.html).

Create a `.env` file in the project directory and add your Census API key:
```CENSUS_API_KEY=your-key-here```
This enables fetching US Census data from the Census API.
:::

```{python}
#| eval: false

# Set your API key into environment (alternative to .env file)
os.environ['CENSUS_API_KEY'] = 'your_api_key_here'
```

## Define Helper Functions

To maintain code reusability and follow DRY (Don't Repeat Yourself) principles, we define helper functions for common operations throughout the analysis.

### Fetch Excel Files from BLS or BTS

This utility function automates downloading data files from federal agencies. It checks if a file already exists locally before attempting to download, avoiding unnecessary network requests and respecting the agencies' servers. The function includes proper HTTP headers to ensure reliable downloads.

```{python}
def fetch_excel(path, url):
    """
    Download Excel file if it doesn't exist locally.

    Parameters:
    -----------
    path : str or Path
        Local file path to save the Excel file
    url : str
        URL to download the Excel file from
    """
    # Convert to Path object if string
    filepath = Path(path)

    # Download file if it doesn't exist
    if not filepath.exists():
        filepath.parent.mkdir(parents=True, exist_ok=True)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers)
        filepath.write_bytes(response.content)
```


### Weighted Spatial Interpolation Function

This custom function performs population-weighted areal interpolation, which is a sophisticated method for transferring data between different geographic units. Traditional spatial joins often use centroids or simple overlaps, which can introduce errors when dealing with irregularly shaped polygons or when population is not evenly distributed.

The `interpolate_pw` function addresses these limitations by:

1. Using point-based weights (like household counts) to represent where population actually lives within each geography
2. Calculating how much of each weight falls within intersected areas
3. Proportionally distributing values based on these weights rather than just area

This approach is particularly important for income data, where we want to account for where households are located, not just land area.

```{python}
def interpolate_pw(from_gdf, to_gdf, weights_gdf, to_id=None, extensive=True,
                   weight_column=None, weight_placement='surface', crs=None):
    """
    Population-weighted areal interpolation between geometries.

    Transfers numeric data from source geometries to target geometries using
    population-weighted interpolation based on point weights (e.g., census blocks).

    Parameters:
    -----------
    from_gdf : GeoDataFrame
        Source geometries with numeric data to interpolate
    to_gdf : GeoDataFrame
        Target geometries to interpolate data to
    weights_gdf : GeoDataFrame
        Weight geometries (e.g., census blocks) used for interpolation.
        If polygons, will be converted to points. Can be the same as to_gdf.
    to_id : str, optional
        Column name for unique identifier in target geometries.
        If None, creates an 'id' column.
    extensive : bool, default True
        If True, return weighted sums (for counts).
        If False, return weighted means (for rates/percentages).
    weight_column : str, optional
        Column name in weights_gdf for weighting (e.g., 'POP', 'HH').
        If None, all weights are equal.
    weight_placement : str, default 'surface'
        How to convert polygons to points: 'surface' or 'centroid'
    crs : str or CRS object, optional
        Coordinate reference system to project all datasets to

    Returns:
    --------
    GeoDataFrame
        Target geometries with interpolated numeric values
    """

    # Input validation
    if not all(isinstance(gdf, gpd.GeoDataFrame) for gdf in [from_gdf, to_gdf, weights_gdf]):
        raise ValueError("All inputs must be GeoDataFrames")

    # Make copies to avoid modifying originals
    from_gdf = from_gdf.copy()
    to_gdf = to_gdf.copy()
    weights_gdf = weights_gdf.copy()

    # Set CRS if provided
    if crs:
        from_gdf = from_gdf.to_crs(crs)
        to_gdf = to_gdf.to_crs(crs)
        weights_gdf = weights_gdf.to_crs(crs)

    # Check CRS consistency
    if not (from_gdf.crs == to_gdf.crs == weights_gdf.crs):
        raise ValueError("All inputs must have the same CRS")

    # Handle to_id
    if to_id is None:
        to_id = 'id'
        to_gdf[to_id] = to_gdf.index.astype(str)

    # Remove conflicting columns
    if to_id in from_gdf.columns:
        from_gdf = from_gdf.drop(columns=[to_id])

    # Create unique from_id
    from_id = 'from_id'
    from_gdf[from_id] = from_gdf.index.astype(str)

    # Handle weight column
    if weight_column is None:
        weight_column = 'interpolation_weight'
        weights_gdf[weight_column] = 1.0
    else:
        # Rename to avoid conflicts
        weights_gdf['interpolation_weight'] = weights_gdf[weight_column]
        weight_column = 'interpolation_weight'

    # Convert weights to points if needed
    if weights_gdf.geometry.geom_type.iloc[0] in ['Polygon', 'MultiPolygon']:
        if weight_placement == 'surface':
            weights_gdf = weights_gdf.copy()
            weights_gdf.geometry = weights_gdf.geometry.representative_point()
        elif weight_placement == 'centroid':
            weights_gdf = weights_gdf.copy()
            weights_gdf.geometry = weights_gdf.geometry.centroid
        else:
            raise ValueError("weight_placement must be 'surface' or 'centroid'")

    # Keep only weight column and geometry
    weight_points = weights_gdf[[weight_column, 'geometry']].copy()

    # Calculate denominators (total weights per source geometry)
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=UserWarning)
        source_weights = gpd.sjoin(from_gdf, weight_points, how='left', predicate='contains')

    denominators = (source_weights.groupby(from_id)[weight_column]
                   .sum()
                   .reset_index()
                   .rename(columns={weight_column: 'weight_total'}))

    # Calculate intersections between from and to
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=UserWarning)
        intersections = gpd.overlay(from_gdf, to_gdf, how='intersection')

    # Filter to keep only polygon intersections
    intersections = intersections[intersections.geometry.geom_type.isin(['Polygon', 'MultiPolygon', 'GeometryCollection'])]

    if len(intersections) == 0:
        raise ValueError("No valid polygon intersections found between source and target geometries")

    # Add intersection ID
    intersections['intersection_id'] = range(len(intersections))

    # Spatial join intersections with weight points to get weights within each intersection
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', category=UserWarning)
        intersection_weights = gpd.sjoin(intersections, weight_points, how='left', predicate='contains')

    # Calculate intersection values (sum of weights per intersection)
    intersection_values = (intersection_weights.groupby('intersection_id')[weight_column]
                         .sum()
                         .reset_index()
                         .rename(columns={weight_column: 'intersection_value'}))

    # Merge back to intersections and keep only unique intersections
    intersections = intersections.merge(intersection_values, on='intersection_id', how='left')
    intersections['intersection_value'] = intersections['intersection_value'].fillna(0)

    # Remove duplicates created by the spatial join
    intersections = intersections.drop_duplicates(subset='intersection_id')

    # Merge with denominators to calculate weight coefficients
    intersections = intersections.merge(denominators, on=from_id, how='left')
    intersections['weight_total'] = intersections['weight_total'].fillna(1)

    # Calculate weight coefficients (intersection weight / total weight in source)
    intersections.loc[intersections['weight_total'] > 0, 'weight_coef'] = (
        intersections['intersection_value'] / intersections['weight_total']
    )
    intersections['weight_coef'] = intersections['weight_coef'].fillna(0)

    # Get numeric columns from source data
    numeric_cols = from_gdf.select_dtypes(include=[np.number]).columns
    # Remove ID columns
    numeric_cols = [col for col in numeric_cols if col not in [from_id]]

    # Prepare intersection data for interpolation
    intersection_data = intersections[[from_id, to_id, 'weight_coef'] + list(numeric_cols)].copy()

    if extensive:
        # For extensive variables: multiply by weight coefficient, then sum by target
        for col in numeric_cols:
            intersection_data[col] = intersection_data[col] * intersection_data['weight_coef']

        interpolated = (intersection_data.groupby(to_id)[list(numeric_cols)]
                       .sum()
                       .reset_index())
    else:
        # For intensive variables: weighted average
        interpolated_data = []
        for target_id in intersection_data[to_id].unique():
            target_data = intersection_data[intersection_data[to_id] == target_id]
            if len(target_data) > 0 and target_data['weight_coef'].sum() > 0:
                weighted_vals = {}
                for col in numeric_cols:
                    weighted_vals[col] = (target_data[col] * target_data['weight_coef']).sum() / target_data['weight_coef'].sum()
                weighted_vals[to_id] = target_id
                interpolated_data.append(weighted_vals)

        interpolated = pd.DataFrame(interpolated_data)

    # Merge with target geometries
    result = to_gdf[[to_id, 'geometry']].merge(interpolated, on=to_id, how='left')

    # Fill NaN values with 0 for missing interpolations
    for col in numeric_cols:
        if col in result.columns:
            result[col] = result[col].fillna(0)

    return result
```

### Median Income Estimation Function

When detailed income distribution data is unavailable at the Census block level, we need to estimate median income from the income bracket counts available in the ACS. This function implements the cumulative distribution method, which:

1. Calculates the cumulative sum of households across income brackets
2. Identifies the bracket containing the 50th percentile (median)
3. Returns the midpoint of that bracket as the estimated median

This provides a reasonable approximation when direct median values are suppressed or unavailable due to data quality concerns.

```{python}
def estimate_median(row, income_columns, midpoints):
    """
    Find median income from household income distribution using cumulative sum method.

    Parameters:
    -----------
    row : DataFrame row
        Row with household counts by income bracket
    income_columns : list
        List of column names with HH counts (HH_LT_10K, HH_10_15K, etc.)
    midpoints : array
        Array of income bracket midpoints

    Returns:
    --------
    float
        Estimated median income
    """
    total_hh = row['HH_TOTAL']

    # Handle edge cases
    if pd.isna(total_hh) or total_hh == 0:
        return np.nan

    cumsum = 0
    for col, midpoint in zip(income_columns, midpoints):
        # Handle missing values in income brackets
        hh_count = row[col] if pd.notna(row[col]) else 0
        cumsum += hh_count

        if cumsum > total_hh / 2:
            return midpoint

    return 300000  # Default for highest bracket
```

## Raw Data Sources

### Transportation Analysis Zone

Load the statewide TAZ shapefile, which contains the geographic boundaries for all transportation analysis zones in Utah. These zones are used in transportation modeling and planning. The data is reprojected to our standard coordinate system and ID columns are formatted as integers for consistency.

```{python}
gdf_TAZ = gpd.read_file(
  r"_data/TAZ/Statewide_TAZ.zip"
).to_crs(PROJECT_CRS)

# Refine the format for ID columns
gdf_TAZ["CO_TAZID"] = gdf_TAZ["CO_TAZID"].astype(int)
gdf_TAZ["SUBAREAID"] = gdf_TAZ["SUBAREAID"].astype(int)
gdf_TAZ["CO_FIPS"] = gdf_TAZ["CO_FIPS"].astype(int)

print(f"Total TAZs: {len(gdf_TAZ)}")
gdf_TAZ.head()
```

The TAZ dataset includes `{python} len(gdf_TAZ)` zones across Utah, covering both metropolitan planning organization (MPO) areas and rural regions.

```{python}
#| eval: false

# Preview the TAZ in an interactive map
gdf_TAZ.explore(column="CO_FIPS", cmap="tab20", legend=True)
```

### Consumer Price Index

The Consumer Price Index (CPI-U-RS) from the Bureau of Labor Statistics provides inflation adjustment factors. This will be useful for comparing income values across different time periods or adjusting historical data to current dollars. The research series (R-CPI-U-RS) is particularly appropriate for longitudinal analysis as it uses consistent methodology throughout the time series.

Download and load the CPI data directly from the BLS website. The data includes annual average CPI values that can be used to adjust income figures for inflation.

Data Source: Consumer Price Index for All Urban Consumers (CPI-U) \[Source: [Bureau of Labor Statistics](https://www.bls.gov/cpi/research-series/r-cpi-u-rs-home.htm)\]

```{python}
# Set file path and URL for CPI data
filepath_cpi = Path("_data/bls/r-cpi-u-rs-allitems.xlsx")
url_cpi = "https://www.bls.gov/cpi/research-series/r-cpi-u-rs-allitems.xlsx"

# Ensure the file exists, Download if not
fetch_excel(path=filepath_cpi, url=url_cpi)

# Read Excel file
df_CPI = pd.read_excel(
    filepath_cpi,
    sheet_name="All items",
    usecols="A:N",
    skiprows=5,
    engine='openpyxl'
)

df_CPI
```

### American Community Survey (ACS) 5-Year Estimates

#### Define Census Variables

Define the specific ACS variables we need for this analysis. Table B19013 provides median household income, while Table B19001 provides the household income distribution across 16 income brackets. These brackets allow us to calculate weighted averages and estimate medians when direct values are unavailable.

```{python}
# Define variables to download
acs_variables = {
    'B19013_001E': 'HH_MED_INC',  # Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars)
    'B19013_001M': 'HH_MED_INC_MOE',  # Margin of Error for Median Household Income
    'B19001_001E': 'HH_TOTAL',  # Total Households
    'B19001_002E': 'HH_LT_10K',  # Less than $10,000
    'B19001_003E': 'HH_10_15K',  # $10,000 to $14,999
    'B19001_004E': 'HH_15_20K',  # $15,000 to $19,999
    'B19001_005E': 'HH_20_25K',  # $20,000 to $24,999
    'B19001_006E': 'HH_25_30K',  # $25,000 to $29,999
    'B19001_007E': 'HH_30_35K',  # $30,000 to $34,999
    'B19001_008E': 'HH_35_40K',  # $35,000 to $39,999
    'B19001_009E': 'HH_40_45K',  # $40,000 to $44,999
    'B19001_010E': 'HH_45_50K',  # $45,000 to $49,999
    'B19001_011E': 'HH_50_60K',  # $50,000 to $59,999
    'B19001_012E': 'HH_60_75K',  # $60,000 to $74,999
    'B19001_013E': 'HH_75_100K',  # $75,000 to $99,999
    'B19001_014E': 'HH_100_125K',  # $100,000 to $124,999
    'B19001_015E': 'HH_125_150K',  # $125,000 to $149,999
    'B19001_016E': 'HH_150_200K',  # $150,000 to $199,999
    'B19001_017E': 'HH_GT_200K'  # $200,000 or more
}
```

#### State Level Data

Retrieve state-level income data for Utah. While we'll primarily use smaller geographies, state-level data provides a useful benchmark for comparison and validation.

```{python}
# Fetch state boundaries from TIGER/Line shapefiles
gdf_ut_bound = states(
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Filter for Utah only
gdf_ut_bound = gdf_ut_bound[gdf_ut_bound['STATEFP'] == str(STATE_FIPS)]

# Fetch Income data from ACS 5-year estimates for Utah
df_ut_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"state:{STATE_FIPS}"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to block group boundaries and transform CRS
gdf_ut_income = gdf_ut_bound[['GEOID', 'STATEFP', 'NAME', 'geometry']].merge(
    df_ut_income, on = "GEOID"
).to_crs(PROJECT_CRS).rename(columns=acs_variables)

# Preview data
gdf_ut_income
```

#### County Level Data

Fetch county-level income data for all 29 counties in Utah. County medians serve as a fallback when block group data is unavailable or unreliable. The ACS provides relatively stable estimates at the county level due to larger sample sizes.

```{python}
# Fetch county boundaries from TIGER/Line shapefiles
gdf_county_bound = counties(
  state=STATE_FIPS,
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Fetch Income data from ACS 5-year estimates for counties in Utah
df_county_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"county:*",
      "in": f"state:{STATE_FIPS}"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to county boundaries and tranform CRS
gdf_county_income = gdf_county_bound[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(
    df_county_income, on = "GEOID"
).to_crs(PROJECT_CRS).rename(columns=acs_variables)

# Preview data
print(f"Total counties: {len(gdf_county_income)}")
gdf_county_income.head()
```

```{python}
# Preview the date in an interactive map
gdf_county_income.explore(column="HH_MED_INC", cmap="YlGnBu", legend=True)
```

The Coefficient of Variation (CV) measures data reliability by comparing the margin of error to the estimate itself. Values below 15% are generally considered reliable, 15-30% should be used with caution, and values above 30% are considered unreliable. This check helps identify counties where data quality may be a concern.

```{python}
# Data Validity Check: Coefficient of Variation (CV) for Median Household Income
gdf_county_income['HH_MED_INC_CV'] = (
  gdf_county_income['HH_MED_INC_MOE'] / 1.645
) / gdf_county_income['HH_MED_INC'] * 100

gdf_county_income[['NAMELSAD', 'HH_MED_INC', 'HH_MED_INC_MOE', 'HH_MED_INC_CV']].sort_values(by='HH_MED_INC_CV', ascending=False).head(10)
```

#### Block Group Level Data

Download income data for all census block groups in Utah. Block groups are the smallest geography for which the ACS publishes detailed income data. Each block group typically contains 600-3,000 people.

```{python}
# Fetch block group boundaries from TIGER/Line shapefiles
gdf_bg_bound = block_groups(
  state=STATE_FIPS,
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Fetch Income data from ACS 5-year estimates for block groups in Utah
df_bg_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"block group:*",
      "in": f"state:{STATE_FIPS} county:*"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to block group boundaries and transform CRS
gdf_bg_income = gdf_bg_bound[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(
    df_bg_income, on = "GEOID"
).to_crs(PROJECT_CRS).rename(columns=acs_variables)

# preview data
print(f"Total block groups: {len(gdf_bg_income)}")
gdf_bg_income.head(10)
```

```{python}
#| eval: false

# Preview the date in an interactive map
gdf_bg_income.explore(column="HH_MED_INC", cmap="YlGnBu", legend=True)
```

Block groups with null median income values or extreme values (≥$250,000) require special handling. These often occur in areas with small populations, high-income enclaves, or data quality issues. For these cases, we'll use either estimated medians from the income distribution or fall back to county values.

```{python}
# Data Validity Check
gdf_bg_income[
    gdf_bg_income["HH_MED_INC"].isna() | (gdf_bg_income["HH_MED_INC"] >= 250000)
]
```

#### Decennial Census 2020 - Census Blocks

Census blocks are the smallest geographic unit used by the Census Bureau, typically containing just a few dozen housing units. While the ACS doesn't provide income data at this level, the 2020 Decennial Census provides precise household counts that we can use as weights for spatial interpolation.

```{python}
# Define variables
dec_variables = {
    'H9_001N': 'HH20'  # Total Number of Households
}

# Get census block geometries
gdf_block_bound = blocks(
    state=STATE_FIPS,
    year=2020,
    cache=True
)

# Download household counts
df_block_hh = get_census(
    dataset="dec/dhc",
    year=2020,
    variables=list(dec_variables.keys()),
    params={
        "for": f"block:*",
        "in": f"state:{STATE_FIPS} county:*"
    },
    return_geoid=True,
    guess_dtypes=True,
)

# Join and rename
gdf_block_hh = gdf_block_bound[['GEOID20', 'geometry']].merge(
    df_block_hh, left_on="GEOID20", right_on="GEOID"
).to_crs(PROJECT_CRS).rename(columns=dec_variables)[['GEOID', 'HH20', 'geometry']]

print(f"Total census blocks: {len(gdf_block_hh)}")
print(f"Blocks with households: {(gdf_block_hh['HH20'] > 0).sum()}")
gdf_block_hh.head()
```

Out of `{python} len(gdf_block_hh)`, total census blocks in Utah, only `{python} (gdf_block_hh['HH20'] > 0).sum()` have households. Many blocks represent unpopulated areas like parks, water bodies, or commercial/industrial zones.

## Lookup Tables

### Income Category Lookup

Create a reference table defining the income brackets used in ACS Table B19001. Each bracket has a lower and upper limit, and we calculate midpoints for median estimation. The highest bracket ($200,000+) uses $300,000 as a reasonable midpoint based on income distribution patterns.

```{python}
lookup_hhinc = pd.DataFrame({
  "Income Category": [
    "HH_LT_10K", "HH_10_15K", "HH_15_20K", "HH_20_25K", "HH_25_30K", "HH_30_35K",
    "HH_35_40K", "HH_40_45K", "HH_45_50K", "HH_50_60K", "HH_60_75K",
    "HH_75_100K", "HH_100_125K", "HH_125_150K", "HH_150_200K", "HH_GT_200K"
  ],
  "Lower Limit": [
    0, 10000, 15000, 20000, 25000, 30000,
    35000, 40000, 45000, 50000, 60000,
    75000, 100000, 125000, 150000, 200000
  ],
  "Upper Limit": [
    9999, 14999, 19999, 24999, 29999, 34999,
    39999, 44999, 49999, 59999, 74999,
    99999, 124999, 149999, 199999, np.inf
  ]
})

# Compute midpoint and round it
lookup_hhinc['Midpoint'] = (
  (lookup_hhinc['Lower Limit'] + lookup_hhinc['Upper Limit']) / 2
).round()

# Replace infinite midpoint (last category) with 300000
lookup_hhinc.loc[np.isinf(lookup_hhinc["Upper Limit"]), "Midpoint"] = 300000

lookup_hhinc
```

### Region Lookup

Define the model spaces (planning regions) used in transportation modeling. UDOT represents statewide planning, while the numbered regions correspond to metropolitan planning organizations and rural planning areas.

```{python}
subareaid_lookup = {
    0: 'UDOT',
    1: 'Wasatch Front',
    2: 'Cache',
    3: 'Dixie',
    4: 'Summit-Wasatch',
    5: 'Iron'
}

subareaid_lookup
```

## Intermediate Data Processing

### Step 1: Process Block Group Income Data

The ACS provides median income directly for most block groups, but some have missing or suppressed values due to small sample sizes or data quality concerns. We need a systematic approach to handle these cases while maintaining data reliability.

Apply the median estimation function to calculate income from the household distribution for each block group. This provides a fallback when the ACS-reported median is unavailable.

```{python}
# Get list of income bracket from lookup table
income_bracket_cols = lookup_hhinc["Income Category"].tolist()

# Get midpoint values from lookup table
midpoints = lookup_hhinc['Midpoint'].values

# Prepare working dataset
df_bg_income_process = gdf_bg_income[[
    'GEOID', 'COUNTYFP', 'NAMELSAD', 'HH_TOTAL', 'HH_MED_INC', 'HH_MED_INC_MOE',
    *income_bracket_cols
]].copy()

# Apply the function to each row to calculate estimated median
df_bg_income_process['EstMedInc'] = df_bg_income_process.apply(
    estimate_median,
    axis=1,  # Apply function row-wise
    income_columns=income_bracket_cols,
    midpoints=midpoints
)

print("Estimated median income calculated")
df_bg_income_process.head()
```

Assign quality flags to each block group based on data availability:

* **Flag 0:** High quality - ACS reported median is available and reasonable (>$0 and <$250,000)
* **Flag 1:** County fallback - No reliable data at block group level, use county median
* **Flag 2:** Estimated - Use calculated median from income distribution

Create the final median income column by selecting the most appropriate value based on the quality flag. This hierarchical approach prioritizes direct ACS estimates but ensures all block groups have a reasonable income value.

```{python}
# Assign data quality flags
conditions = [
    # Condition 1: Does the block group have a good ACS reported median?
    (df_bg_income_process['HH_MED_INC'].notna()) &           # Median exists (not missing)
    (df_bg_income_process['HH_MED_INC'] > 0) &               # Median is positive
    (df_bg_income_process['HH_MED_INC'] < 250001),           # Median is reasonable (not extreme)

    # Condition 2: Do we have an estimated median from distribution?
    df_bg_income_process['EstMedInc'].notna()                # Estimated median exists
]

df_bg_income_process['Flag'] = np.select(
    conditions,      # List of conditions to check
    [0, 2],         # Values to assign: 0 if condition 1 True, 2 if condition 2 True
    default=1       # If neither condition is True, assign 1
)

# Join county median for fallback
df_bg_income_process = df_bg_income_process.merge(
    gdf_county_income[['COUNTYFP', 'HH_MED_INC']].rename(columns={'HH_MED_INC': 'County_MedInc'}),
    on='COUNTYFP',
    how='left'
)

# Create final median income column based on flag
medinc_conditions = [
    df_bg_income_process['Flag'] == 0,
    df_bg_income_process['Flag'] == 1,
    df_bg_income_process['Flag'] == 2
]

medinc_choices = [
    df_bg_income_process['HH_MED_INC'], # ACS Reported block group household Income
    df_bg_income_process['County_MedInc'], # County fallback if none is true
    df_bg_income_process['EstMedInc'] # Estimated median fallback if ACS reported doesnt exist
]

df_bg_income_process['Median Income'] = np.select(medinc_conditions, medinc_choices, default=np.nan)
```

```{python}
# Create final processed dataset
df_bg_income_process = df_bg_income_process[[
    'GEOID', 'COUNTYFP', 'NAMELSAD', 'HH_TOTAL', 'HH_MED_INC',
    'EstMedInc', 'County_MedInc', 'Flag', 'Median Income'
]].copy()

# Add GEOID_BG column
df_bg_income_process['GEOID_BG'] = df_bg_income_process['GEOID']

# Display summary
df_bg_income_process.head(10)
```

### Step 2: Prepare Block Group Geometries with Income

Combine the block group geometries with the processed income data. This creates a spatial dataset where each block group polygon contains its estimated median income and quality flag.

```{python}
# Get block group geometries with median income
gdf_bg_with_income = gdf_bg_bound[['GEOID', 'geometry']].merge(
    df_bg_income_process[['GEOID_BG', "NAMELSAD", "HH_TOTAL", "Median Income", "Flag"]],
    left_on='GEOID',
    right_on='GEOID_BG',
    how='inner'
).to_crs(PROJECT_CRS)

print(f"Block groups with income data: {len(gdf_bg_with_income)}")
gdf_bg_with_income.head()
```

```{python}
#| eval: false

# Explore the processed block group median income data
gdf_bg_with_income.explore(
    column="Median Income",
    cmap="YlGnBu",
    legend=True,
    tooltip=["GEOID", "NAMELSAD", "HH_TOTAL", "Median Income", "Flag"]
)
```

### Step 3: Prepare Census Blocks with Household Counts

Extract the block group identifier from each census block's GEOID. The first 12 characters of a block GEOID uniquely identify its parent block group, allowing us to link blocks to their income data.

```{python}
# Extract Block Group GEOID from Block GEOID (first 12 characters)
gdf_block_hh['GEOID_BG'] = gdf_block_hh['GEOID'].str[:12]
```

Join the median income from block groups to individual census blocks. Each block inherits the income characteristics of its parent block group, as income data is not available at the block level.

::: {.callout-note}
Median income is transferred directly from block groups to blocks via a simple join on `GEOID_BG`. Each block inherits the median income of its parent block group.
:::

```{python}
# Join median income from block groups to blocks
gdf_block_hh_income = gdf_block_hh.merge(
    df_bg_income_process[['GEOID_BG', 'Median Income']],
    on='GEOID_BG',
    how='left'
)

print(f"Blocks after join: {len(gdf_block_hh_income)}")
print(f"Blocks with median income: {gdf_block_hh_income['Median Income'].notna().sum()}")
gdf_block_hh_income.head()
```

### Step 4: Weighted Interpolation of 2023 Households to Blocks

The 2020 Decennial Census provides precise household counts, but we need 2023 estimates to match our income data year. This step uses spatial interpolation to distribute 2023 household totals from block groups to individual blocks, weighted by the 2020 block-level household distribution.

Use the weighted interpolation function to proportionally allocate 2023 household totals from block groups to census blocks. The 2020 household counts serve as weights, assuming the relative distribution of households among blocks remains similar. This produces estimated 2023 household counts at the block level.

::: {.callout-note}
This step uses the `interpolate_pw` function to distribute 2023 ACS household totals from block groups to census blocks, weighted by 2020 household counts. We use `extensive=True` because household count is an extensive variable (sum).
:::

```{python}
# Use weighted interpolation to transfer HH_TOTAL from BG to blocks
gdf_blocks_hh23 = interpolate_pw(
    from_gdf=gdf_bg_with_income[['GEOID_BG', 'HH_TOTAL', 'geometry']],
    to_gdf=gdf_block_hh[['GEOID', 'geometry']],
    weights_gdf=gdf_block_hh[['GEOID', 'HH20', 'geometry']],
    to_id='GEOID',
    extensive=True,  # Extensive variable - sum households
    weight_column='HH20',
    weight_placement='centroid',
    crs=PROJECT_CRS
)

# Merge interpolated households with the blocks that have median income
gdf_block_hh_income = gdf_block_hh_income.merge(
    gdf_blocks_hh23[['GEOID', 'HH_TOTAL']].rename(columns={'HH_TOTAL': 'HH23'}),
    on='GEOID',
    how='left'
)

# Round and convert to integer
gdf_block_hh_income['HH23'] = gdf_block_hh_income['HH23'].round(0).astype(int).fillna(0)

print(f"Total 2023 HH in blocks: {gdf_block_hh_income['HH23'].sum():,.0f}")
gdf_block_hh_income.head()
```

After interpolation, the total estimated 2023 households across all blocks should closely match the ACS block group totals.

### Step 5: Polygon-to-Polygon Intersection between Blocks and TAZ

Census blocks and TAZ boundaries don't align perfectly - blocks may span multiple TAZs and TAZs may contain partial blocks. A polygon overlay operation identifies all intersections and preserves the exact geometry of these overlaps.

Perform a geometric intersection between census blocks and TAZ boundaries. This creates new polygons representing every unique combination of block and TAZ, along with the attributes from both datasets.

::: {.callout-note}
Instead of using spatial join with centroids, we use polygon overlay to capture the actual intersection between census blocks and TAZs. This preserves spatial relationships when blocks cross TAZ boundaries.
:::

```{python}
# Perform polygon-to-polygon intersection
gdf_block_taz_intersection = gpd.overlay(
    gdf_block_hh_income[['GEOID', 'GEOID_BG', 'HH20', 'HH23', 'Median Income', 'geometry']],
    gdf_TAZ[['CO_TAZID', 'SUBAREAID', 'CO_FIPS', 'CO_NAME', 'geometry']],
    how='intersection'
)

print(f"Total intersected polygons: {len(gdf_block_taz_intersection):,}")
print(f"Unique blocks in intersections: {gdf_block_taz_intersection['GEOID'].nunique():,}")
print(f"Unique TAZs in intersections: {gdf_block_taz_intersection['CO_TAZID'].nunique():,}")
gdf_block_taz_intersection.head()
```

The intersection produces more records than either input dataset because blocks crossing TAZ boundaries create multiple intersection polygons.

### Step 6: Calculate Intersection Areas and Proportional Households

When a census block spans multiple TAZs, we need to distribute its households proportionally based on the area of each intersection. This assumes households are evenly distributed within each block.

Calculate the area of each intersection polygon and compare it to the original block area. The ratio determines what proportion of the block's households fall within each TAZ.

```{python}
# Calculate area of each intersected polygon
gdf_block_taz_intersection['intersection_area'] = gdf_block_taz_intersection.geometry.area

# Calculate original block areas
block_areas = gdf_block_hh_income[['GEOID', 'geometry']].copy()
block_areas['block_area'] = block_areas.geometry.area
block_areas = block_areas[['GEOID', 'block_area']]

# Merge block areas
gdf_block_taz_intersection = gdf_block_taz_intersection.merge(
    block_areas,
    on='GEOID',
    how='left'
)

# Calculate proportion of block area within each intersection
gdf_block_taz_intersection['area_proportion'] = np.where(
    gdf_block_taz_intersection['block_area'] > 0,
    gdf_block_taz_intersection['intersection_area'] / gdf_block_taz_intersection['block_area'],
    0
)
```

Distribute households to each intersection based on the area proportion. For example, if 30% of a block's area falls in one TAZ and 70% in another, we assign households proportionally.

```{python}

# Distribute households proportionally based on intersection area
gdf_block_taz_intersection['HH23_intersect'] = (
    gdf_block_taz_intersection['HH23'] * gdf_block_taz_intersection['area_proportion']
).round(0)

print(f"Total HH23 after proportional distribution: {gdf_block_taz_intersection['HH23_intersect'].sum():,.0f}")
gdf_block_taz_intersection.head()
```

The total households after proportional distribution should match the sum before distribution, confirming that no households were lost or duplicated in the process.

### Step 7: Calculate TAZ Median Income using Weighted Mean

Rather than simply averaging block-level incomes, we need to calculate a household-weighted mean for each TAZ. This ensures that blocks with more households have more influence on the TAZ median.

For each intersection polygon, calculate the product of households and median income. This weighted value will be used to compute the household-weighted mean income for each TAZ.

```{python}
# Calculate weighted income (HH * Median Income) for each intersected polygon
gdf_block_taz_intersection['HH_MedInc_Product'] = (
    gdf_block_taz_intersection['HH23_intersect'] * gdf_block_taz_intersection['Median Income']
)
```

Group all intersections by TAZ and calculate the weighted mean income. Divide the sum of weighted income products by the sum of households to get the final TAZ median. For TAZs with zero households, use the simple median of block incomes as a fallback.

```{python}
# Group by TAZ and calculate weighted mean
df_taz_aggregated = gdf_block_taz_intersection.groupby('CO_TAZID').agg({
    'SUBAREAID': 'first',
    'CO_FIPS': 'first',
    'CO_NAME': 'first',
    'HH23_intersect': 'sum',
    'HH_MedInc_Product': 'sum',
    'Median Income': 'median'  # For fallback if HH23 = 0
}).reset_index()

# Rename column for clarity
df_taz_aggregated = df_taz_aggregated.rename(columns={'HH23_intersect': 'HH23'})

# Calculate TAZ median income using weighted mean
df_taz_aggregated['MedInc'] = np.where(
    df_taz_aggregated['HH23'].notna() & (df_taz_aggregated['HH23'] > 0),
    (df_taz_aggregated['HH_MedInc_Product'] / df_taz_aggregated['HH23']).round(0),
    df_taz_aggregated['Median Income'].round(0)
)

print(f"\nTotal TAZs: {len(df_taz_aggregated)}")
print(f"TAZs with median income: {df_taz_aggregated['MedInc'].notna().sum()}")
print(f"TAZs with NULL median income: {df_taz_aggregated['MedInc'].isna().sum()}")
df_taz_aggregated.head(10)
```

### Step 8: Handle Remaining NULL Values (County Fallback)

Despite our multi-step process, some TAZs may still lack income estimates, typically in unpopulated areas or areas with data quality issues. County-level medians provide a reasonable final fallback.

Check for any TAZs with missing median income values and fill them with their county's median. This ensures every TAZ has an income estimate, even if it's less precise than block-level data.

```{python}
# Check if any TAZs still have NULL values
if df_taz_aggregated['MedInc'].isna().any():
    print(f"Applying county median fallback for {df_taz_aggregated['MedInc'].isna().sum()} TAZs...")

    # Get county medians
    county_medians = gdf_county_income[['COUNTYFP', 'HH_MED_INC']].copy()
    county_medians['CO_FIPS'] = county_medians['COUNTYFP'].astype(int)
    county_medians = county_medians.rename(columns={'HH_MED_INC': 'County_MedInc'})

    # Merge with TAZ data
    df_taz_aggregated = df_taz_aggregated.merge(
        county_medians[['CO_FIPS', 'County_MedInc']],
        on='CO_FIPS',
        how='left'
    )

    # Fill NULL values with county median
    df_taz_aggregated['MedInc'] = df_taz_aggregated['MedInc'].fillna(
        df_taz_aggregated['County_MedInc']
    )

    print(f"After county fallback - TAZs with NULL: {df_taz_aggregated['MedInc'].isna().sum()}")
else:
    print("No NULL values found - all TAZs have median income!")
```

## Results & Visualizations

### TAZ Median Income Summary

The final dataset contains median income estimates for all `{python} len(gdf_TAZ)` TAZs in Utah, along with estimated 2023 household counts. These values can be used directly in transportation models and planning analyses.

```{python}
# Create final TAZ output
df_taz_income = df_taz_aggregated[[
    'CO_TAZID',
    'SUBAREAID',
    'CO_FIPS',
    'CO_NAME',
    'MedInc',
    'HH23'
]].copy()

# Sort by CO_TAZID
df_taz_income = df_taz_income.sort_values('CO_TAZID').reset_index(drop=True)

print("="*80)
print("FINAL TAZ MEDIAN INCOME RESULTS")
print("="*80)
df_taz_income
```

TAZs with zero households typically represent unpopulated areas, employment centers, or future development zones. Their assigned income values (from county fallbacks) should be used cautiously.

```{python}
# Display TAZs with no households
no_hh_tazs = df_taz_income[df_taz_income['HH23'] == 0]
if len(no_hh_tazs) > 0:
    print(f"\nTAZs with no households: {len(no_hh_tazs)}")
    print(no_hh_tazs[['CO_TAZID', 'CO_NAME', 'MedInc', 'HH23']].head(10))
```

### Interactive Map: TAZ Median Income

Visualize the spatial distribution of median income across Utah's TAZs. The map reveals clear geographic patterns, with higher incomes concentrated in suburban areas and lower incomes in urban cores and rural regions.

```{python}
# Visualize the distribution of TAZ median incomes
gdf_TAZ[['CO_TAZID', 'geometry']].merge(
    df_taz_income[['CO_TAZID', 'SUBAREAID', 'CO_FIPS', 'CO_NAME', 'MedInc', 'HH23']],
    on='CO_TAZID',
    how='left'
).explore(
    column="MedInc",
    cmap="YlGnBu",
    legend=True,
    tooltip=["CO_TAZID", "CO_NAME", "MedInc", "HH23"],
    popup=True
)
```

### Regional Median Income Summary

Aggregate TAZ-level data to calculate household-weighted median income for each planning region. These regional values are particularly useful for comparative analysis and regional planning efforts.

```{python}
# Calculate regional weighted medians
df_regional_aggregated = df_taz_aggregated.groupby('SUBAREAID').agg({
    'HH23': 'sum',
    'HH_MedInc_Product': 'sum'
}).reset_index()

df_regional_aggregated['MedInc'] = np.where(
    df_regional_aggregated['HH23'] > 0,
    (df_regional_aggregated['HH_MedInc_Product'] / df_regional_aggregated['HH23']).round(0),
    np.nan
).astype(int)

df_regional_aggregated['Model Space'] = df_regional_aggregated['SUBAREAID'].map(subareaid_lookup)

df_regional_income = df_regional_aggregated[[
    'SUBAREAID',
    'Model Space',
    'MedInc',
    'HH23'
]].copy().sort_values('SUBAREAID').reset_index(drop=True)

print("\n" + "="*80)
print("REGIONAL MEDIAN INCOME SUMMARY")
print("="*80)
df_regional_income
```

The Summit-Wasatch region, which includes resort communities, shows the highest median income, followed by Wasatch Front region (containing the largest metropolitan areas). Rural regions generally have lower median incomes, reflecting different economic structures and costs of living.

### Result Validation

To ensure our methodology produces reasonable results, we compare our 2023 estimates with previously calculated 2019 values. We expect median incomes to increase due to both inflation and real wage growth.

Load the 2019 regional median income data from the previous analysis for comparison.

```{python}
# Compare county median income with 2019 calculations
df_regional_income_2019 = pd.read_excel(
  r"_archive/_Source - TAZ & Regional Median Income - 2022-03-17.xlsb",
  sheet_name="Export_MedInc",
  usecols="H:J",
  skiprows=4,
  nrows=6,
  engine="pyxlsb"
)

df_regional_income_2019[['MedInc_2019']] = df_regional_income_2019[['MedInc']].astype(int)
```

Calculate the absolute and percentage change between 2019 and 2023. The changes should be consistent with inflation rates and economic growth over this period. Typical increases of 20-30% over four years align with the combination of ~20% cumulative inflation plus modest real wage growth during this period.

```{python}
# Merge 2019 and 2023 data
df_regional_comparison = df_regional_income[['SUBAREAID', 'Model Space', 'MedInc']].rename(columns={
    'MedInc': 'MedInc_2023'
}).merge(
    df_regional_income_2019[['SUBAREAID', 'MedInc_2019']],
    on='SUBAREAID',
    how='left'
)

# Calculate absolute and percent change
df_regional_comparison['Absolute_Change'] = (
    df_regional_comparison['MedInc_2023'] - df_regional_comparison['MedInc_2019']
)

df_regional_comparison['Percent_Change'] = (
    (df_regional_comparison['MedInc_2023'] - df_regional_comparison['MedInc_2019'])
    / df_regional_comparison['MedInc_2019'] * 100
).round(2)



# Display comparison
print("\n" + "="*80)
print("REGIONAL MEDIAN INCOME COMPARISON: 2019 vs 2023")
print("="*80)
df_regional_comparison
```

The comparison shows consistent increases across all regions, with percentage changes ranging from approximately 20-30%. This aligns with economic trends during 2019-2023, including significant inflation, particularly in 2021-2022. The Wasatch Front shows slightly lower percentage growth, which may reflect its already-high base, while smaller regions show more variation due to sample size effects.

### Chart

#### County Median Income Distribution

This scatter plot ranks Utah's 29 counties by median household income and visualizes the relationship between rank and income level. Bubble size represents total households in each county, highlighting the population concentration in metropolitan areas.

```{python}
# Create plotting dataframe
df_plot = gdf_county_income[['NAMELSAD', 'HH_MED_INC', 'HH_TOTAL']].copy()

# Rank counties by median household income
df_plot["Rank"] = df_plot["HH_MED_INC"].rank(
    method="min",
    ascending=False
).astype(int)

# Create Region column based on MPO counties
mpo_counties = ['Summit County', 'Wasatch County', 'Davis County', 'Salt Lake County',
                'Utah County', 'Weber County', 'Washington County', 'Cache County', 'Iron County']

df_plot['Region'] = df_plot['NAMELSAD'].apply(lambda x: 'MPO' if x in mpo_counties else 'Non-MPO')

# Define color mapping
color_map = {'MPO': '#843C0C', 'Non-MPO': '#4472C4'}
```

```{python}
#| out-width: "100%"

# Set style
sns.set_style("whitegrid")

# Create figure
# plt.figure(figsize=(10, 8))

# Create scatter plot
sns.scatterplot(
    data=df_plot,
    x='Rank',
    y='HH_MED_INC',
    size='HH_TOTAL',
    sizes=(100, 10000),
    hue='Region',
    palette=color_map,
    alpha=0.3,
    edgecolor=df_plot['Region'].map(color_map),
    linewidth=2.5,
    legend=False
)

# Add smooth trend line
sns.regplot(
    data=df_plot,
    x='Rank',
    y='HH_MED_INC',
    scatter=False,
    order=3,
    ci=None,
    color='red',
    line_kws={'linestyle':':', 'dashes':(1, 1), 'linewidth':2, 'alpha':0.8}
)

# Create list to store text objects
texts = []

# Add labels for each county
for idx, row in df_plot.iterrows():
    county_name = row['NAMELSAD'].replace(' County', '').upper()
    income_label = f"${row['HH_MED_INC']:,.0f}"
    label = f"{county_name}\n{income_label}"
    label_color = color_map[row['Region']]

    text = plt.annotate(
        label,
        xy=(row['Rank'], row['HH_MED_INC']),
        xytext=(row['Rank'], row['HH_MED_INC']),
        textcoords='data',
        ha='center',
        va='center',
        fontsize=7,
        fontweight='bold',
        color=label_color
    )
    texts.append(text)

# Adjust text positions to avoid overlap
adjust_text(
    texts,
    arrowprops=dict(arrowstyle='->', color='gray', lw=0.5, alpha=0.6, shrinkA=10, shrinkB=10),
    lim=500
)

# Add horizontal line at state median
state_avg = gdf_ut_income['HH_MED_INC'].values[0]
plt.axhline(y=state_avg, color='#C00000', linestyle='--', linewidth=2, alpha=0.4)

# Add text annotation for state average
plt.text(29.5, state_avg + 1500, f'Utah State Median: ${state_avg:,.0f}',
         fontsize=10, color='#C00000', ha='right', va='bottom', fontweight='bold')

# Set axis limits and ticks
plt.xlim(0, 30)
plt.ylim(0, 150000)
plt.xticks(range(0, 31, 5))

# Format y-axis
plt.gca().yaxis.set_major_locator(ticker.MultipleLocator(20000))
plt.gca().yaxis.set_minor_locator(ticker.MultipleLocator(10000))
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x:,.0f}'))

# Customize grid
plt.grid(True, which='major', axis='y', alpha=0.4, linewidth=0.8, color='gray')
plt.grid(True, which='minor', axis='y', alpha=0.2, linewidth=0.5, color='gray')

# Set labels and title
plt.xlabel('Rank', fontsize=12, fontweight='bold', color='#595959')
plt.ylabel('Median Household Income', fontsize=12, fontweight='bold', color='#595959')
plt.title('Median Income in 2023 Dollars', fontsize=16, fontweight='bold', pad=20, color='#595959')
plt.gca().text(0.5, 1.02, 'Source: 2019-2023 American Community Survey',
               transform=plt.gca().transAxes, ha='center', fontsize=10.5,
               color='#595959', style='italic')

plt.tight_layout()
plt.show()
```

The chart distinguishes between MPO (Metropolitan Planning Organization) and non-MPO counties, revealing that most high-income counties are part of MPO regions. The curved trend line shows that income differences between top-ranked counties are larger than differences among lower-ranked counties. The state median provides context for interpreting individual county values.

#### County vs Block Group Median Comparison

This validation chart compares county-level median income (reported directly by ACS) with the median of all block group medians within each county. If our block group data is reliable, these two values should be highly correlated.

```{python}
#| out-width: "100%"

# Calculate median of block group median incomes for each county
bg_median_by_county = gdf_bg_income[
    gdf_bg_income['HH_MED_INC'].notna()
].groupby('COUNTYFP')['HH_MED_INC'].median().reset_index()
bg_median_by_county.columns = ['COUNTYFP', 'BG_MedianOfMedians']

# Merge with county data
plot_data = gdf_county_income[['COUNTYFP', 'NAMELSAD', 'HH_MED_INC']].merge(
    bg_median_by_county,
    on='COUNTYFP',
    how='left'
)

# Filter out missing values
plot_data = plot_data[
    (plot_data['HH_MED_INC'].notna()) &
    (plot_data['BG_MedianOfMedians'].notna())
].copy()

# Set style
sns.set_style("whitegrid")

# Create figure
# plt.figure(figsize=(10, 8))

# Create scatter plot
sns.scatterplot(
    data=plot_data,
    x='BG_MedianOfMedians',
    y='HH_MED_INC',
    alpha=0.6,
    s=150,
    color='#4472C4',
    edgecolor='#4472C4',
    linewidth=2,
    legend=False
)

# Add regression line
sns.regplot(
    data=plot_data,
    x='BG_MedianOfMedians',
    y='HH_MED_INC',
    scatter=False,
    color='#C00000',
    line_kws={
        'linestyle': '--',
        'linewidth': 2,
        'alpha': 0.7,
        'label': 'Trend line'
    },
    ci=None
)

# Add diagonal reference line (y=x)
min_val = min(plot_data['BG_MedianOfMedians'].min(), plot_data['HH_MED_INC'].min())
max_val = max(plot_data['BG_MedianOfMedians'].max(), plot_data['HH_MED_INC'].max())
plt.plot([min_val, max_val], [min_val, max_val],
         color='gray',
         linestyle=':',
         linewidth=1.5,
         alpha=0.5,
         label='y=x (perfect match)')

# Add county labels
for idx, row in plot_data.iterrows():
    county_name = row['NAMELSAD'].replace(' County', '')
    plt.annotate(
        county_name,
        xy=(row['BG_MedianOfMedians'], row['HH_MED_INC']),
        xytext=(5, 5),
        textcoords='offset points',
        fontsize=8,
        alpha=0.7
    )

# Format axes
plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))

# Labels and title
plt.xlabel('Median of Block Group Median Incomes', fontsize=12, fontweight='bold', color='#595959')
plt.ylabel('County Median Income', fontsize=12, fontweight='bold', color='#595959')
plt.title('County Median Income vs Median of Block Group Median Incomes',
          fontsize=14, fontweight='bold', pad=15, color='#595959')

# Add legend
plt.legend(loc='lower right', fontsize=9)

# Add correlation coefficient
corr = plot_data['BG_MedianOfMedians'].corr(plot_data['HH_MED_INC'])
plt.text(0.02, 0.98, f'Correlation: {corr:.3f}\nn={len(plot_data)} counties',
         transform=plt.gca().transAxes,
         fontsize=10,
         verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

A strong correlation (close to 1.0) indicates that block group data accurately reflects county-level patterns. Points falling near the diagonal reference line (y=x) show counties where the two measures align closely. Deviations suggest either data quality issues at the block group level or genuine within-county variation in how income is distributed.

#### 2019 vs 2023 Block Group Income Distribution

```{python}
# Fetch block group boundaries from TIGER/Line shapefiles
gdf_bg_bound19 = block_groups(
  state=STATE_FIPS,
  year=2019,
  cache=True
).to_crs(PROJECT_CRS)

# Fetch Income data from ACS 5-year estimates for block groups in Utah
df_bg_income19 = get_census(
  dataset="acs/acs5",
  year=2019,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"block group:*",
      "in": f"state:{STATE_FIPS} county:*"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to block group boundaries and transform CRS
gdf_bg_income19 = gdf_bg_bound19[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(
    df_bg_income19, on = "GEOID"
).to_crs(PROJECT_CRS).rename(columns=acs_variables)

gdf_bg_income19.head(10)
```

Compare the distribution of block group median incomes between the 2015-2019 and 2019-2023 ACS periods. The rightward shift of the 2023 distribution reflects both inflation and real income growth.

```{python}
# Prepare data: Combine 2019 and 2023 block group income data
df_bg_comparison = pd.DataFrame({
    'Median Income': pd.concat([
        gdf_bg_income19['HH_MED_INC'].dropna(),
        gdf_bg_income['HH_MED_INC'].dropna()
    ]),
    'Year': ['2019'] * gdf_bg_income19['HH_MED_INC'].notna().sum() +
            ['2023'] * gdf_bg_income['HH_MED_INC'].notna().sum()
})
```

```{python}
#| out-width: "100%"

# Plot: Overlaid histograms with 20 bins
sns.set_style("whitegrid")
# plt.figure(figsize=(10, 8))

sns.histplot(
    data=df_bg_comparison,
    x='Median Income',
    hue='Year',
    bins=20,
    palette={'2019': '#843C0C', '2023': '#4472C4'},
    alpha=0.6,
    stat='count',
    element='bars',
    kde=False
)

plt.title('Block Group Median Income Distribution\n2019 vs 2023',
          fontweight='bold', fontsize=14, pad=15)
plt.xlabel('Median Household Income ($)', fontweight='bold', fontsize=11)
plt.ylabel('Number of Block Groups', fontweight='bold', fontsize=11)
plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
plt.tight_layout()
plt.show()
```

The overlaid histograms reveal how the entire income distribution shifted upward. The 2023 distribution shows fewer block groups in the lowest income brackets and more in middle and upper brackets, consistent with the inflationary period and wage increases during this time.

#### 2019 vs 2023 Regional Median Income Comparison

The grouped bar chart directly compares regional median incomes between 2019 and 2023, making the magnitude of change clearly visible across all planning regions.

```{python}
# Prepare long format data
df_long = df_regional_comparison.melt(
    id_vars=['Model Space'],
    value_vars=['MedInc_2019', 'MedInc_2023'],
    var_name='Year',
    value_name='Median Income'
)
df_long['Year'] = df_long['Year'].str.replace('MedInc_', '')
```

```{python}
#| out-width: "100%"

# Plot 1: Grouped bar chart
sns.set_style("whitegrid")
# plt.figure(figsize=(10, 8))
ax = sns.barplot(data=df_long, x='Model Space', y='Median Income', hue='Year',
                 palette={'2019': '#843C0C', '2023': '#4472C4'})
plt.title('Regional Median Income\n2019 vs 2023', fontweight='bold', pad=15)
plt.xticks(rotation=45, ha='right')
plt.ylabel('Median Household Income ($)', fontweight='bold')
plt.xlabel('Model Space', fontweight='bold')
ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
ax.bar_label(ax.containers[0], labels=[f'${x:,.0f}' for x in ax.containers[0].datavalues], fontsize=8)
ax.bar_label(ax.containers[1], labels=[f'${x:,.0f}' for x in ax.containers[1].datavalues], fontsize=8)
plt.tight_layout()
plt.show()
```

```{python}
#| out-width: "100%"

# Plot 2: Percent change
colors = ['green' if x >= 0 else 'red' for x in df_regional_comparison['Percent_Change']]
sns.set_style("whitegrid")
# plt.figure(figsize=(10, 8))
sns.barplot(data=df_regional_comparison, y='Model Space', x='Percent_Change',
            hue='Model Space', palette=colors, legend=False)
plt.title('Percent Change in Median Income\n(2019 to 2023)', fontweight='bold', pad=15)
plt.xlabel('Percent Change (%)', fontweight='bold')
plt.ylabel('')
plt.axvline(x=0, color='black', linewidth=0.8)

# Add percent labels
for i, val in enumerate(df_regional_comparison['Percent_Change']):
    plt.text(val, i, f' {val:.1f}%', ha='left' if val >= 0 else 'right',
            va='center', fontweight='bold', fontsize=9)

plt.tight_layout()
plt.show()
```

All regions show substantial increases, with the absolute dollar increases being largest in already-high-income regions like Wasatch Front and Summit-Wasatch. The percentage change chart normalizes these increases, showing that rural regions experienced similar or sometimes higher percentage growth despite lower baseline incomes.

## Export Results

Save the final TAZ and regional median income datasets as CSV files for use in the travel demand model and other planning applications. These files can be joined to other datasets using TAZ IDs or region IDs.

```{python}
# Create output directory if it doesn't exist
output_dir = Path("_output")
output_dir.mkdir(parents=True, exist_ok=True)

# Export to CSV
df_taz_income.to_csv(
    output_dir / "taz_median_income.csv",
    index=False
)

# Export to CSV
df_regional_income.to_csv(
    output_dir / "regional_median_income.csv",
    index=False
)
```

The exported files provide ready-to-use income estimates that reflect the most recent available data (2019-2023 ACS) and have been carefully processed to handle data quality issues while preserving spatial accuracy.

::: {.callout-tip title="Download the output files:"}
[taz_median_income.csv](./_output/taz_median_income.csv) | [regional_median_income.csv](./_output/regional_median_income.csv)
:::
