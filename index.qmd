---
title: TAZ & Regional Median Income
subtitle: Calculating Median Income for Transportation Analysis Zones (TAZ)
description: This notebook replicates the analysis from the '_Source - TAZ & Regional Median Income - 2022-03-17.xlsb' using the updated household and household income data from the 2019-2023 American Community Survey.
author:
 - name: Pukar Bhandari
   email: pukar.bhandari@wfrc.utah.gov
   affiliation:
     - name: Wasatch Front Regional Council
       url: "https://wfrc.utah.gov/"
date: "2025-10-08"

execute:
   eval: true
jupyter: python3

format:
  html:
    theme:
        light: flatly
        dark: darkly
    toc: true
    number-sections: true
    number-depth: 2
    html-math-method: katex
    code-link: true
    code-tools: true
    code-fold: true
    code-summary: "Show the code"
    # fig-width: 8
    # fig-height: 5
    # out-width: "100%"
    # fig-align: center
    resources:
    - "_output/*.csv"

title-block-banner: true
---

## Environment Setup

### Load Libraries

``` python
!conda install -c conda-forge numpy pandas geopandas matplotlib seaborn python-dotenv openpyxl
!pip install pygris
```

```{python}
# For Analysis
import numpy as np
import pandas as pd
import geopandas as gpd

# For Visualization
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
import seaborn as sns
from adjustText import adjust_text

# Census data query libraries & modules
from pygris import blocks, block_groups, counties, states
from pygris.helpers import validate_state, validate_county
from pygris.data import get_census, get_lodes

# misc
import datetime
import os
from pathlib import Path
import requests

from dotenv import load_dotenv
load_dotenv()
```

### Environment Variables

```{python}
PROJECT_CRS = "EPSG:3566"  # NAD83 / UTM zone 12N
```

::: {.callout-tip}
**Need a Census API key?** Get one for free at [census.gov/developers](https://api.census.gov/data/key_signup.html).

Create a `.env` file in the project directory and add your Census API key:
```CENSUS_API_KEY=your-key-here```
This enables fetching US Census data from the Census API.
:::

```{python}
#| eval: false

# Set your API key into environment
os.environ['CENSUS_API_KEY'] = 'your_api_key_here'
```

```{python}
STATE_FIPS = validate_state("UT")
```

## Raw Data Sources

### Transportation Analysis Zone

```{python}
gdf_TAZ = gpd.read_file(
  r"_data/TAZ/Statewide_TAZ.zip"
).to_crs(PROJECT_CRS)

gdf_TAZ.head()
```

```{python}
#| eval: false

# Preview the TAZ in an interactive map
gdf_TAZ.explore(column="CO_TAZID", cmap="tab20", legend=True)
```

### Consumer Price Index

Data Source: Consumer Price Index for All Urban Consumers (CPI-U) \[Source: [Bureau of Labor Statistics](https://www.bls.gov/cpi/research-series/r-cpi-u-rs-home.htm)\]

```{python}
# Ensure file exists
filepath_cpi = Path("_data/bls/r-cpi-u-rs-allitems.xlsx")
if not filepath_cpi.exists():
    filepath_cpi.parent.mkdir(parents=True, exist_ok=True)
    response = requests.get("https://www.bls.gov/cpi/research-series/r-cpi-u-rs-allitems.xlsx")
    filepath_cpi.write_bytes(response.content)

# Read Excel file directly from URL
df_CPI = pd.read_excel(
  filepath_cpi, # File path
  sheet_name="All items",
  usecols="A:N", # TODO: update cols later for new data
  skiprows=5,    # Skip the first two rows
  engine='openpyxl'  # Specify engine
)

# display the data
df_CPI
```

### ACS_5YR_HouseholdIncome

#### Set Census Variables

```{python}
# Define variables to download
acs_variables = {
    'B19013_001E': 'HH_MED_INC',  # Median Household Income in the Past 12 Months (in 2023 Inflation-Adjusted Dollars)
    'B19013_001M': 'HH_MED_INC_MOE',  # Margin of Error for Median Household Income
    'B19001_001E': 'HH_TOTAL',  # Total Households
    'B19001_002E': 'HH_LT_10K',  # Less than $10,000
    'B19001_003E': 'HH_10_15K',  # $10,000 to $14,999
    'B19001_004E': 'HH_15_20K',  # $15,000 to $19,999
    'B19001_005E': 'HH_20_25K',  # $20,000 to $24,999
    'B19001_006E': 'HH_25_30K',  # $25,000 to $29,999
    'B19001_007E': 'HH_30_35K',  # $30,000 to $34,999
    'B19001_008E': 'HH_35_40K',  # $35,000 to $39,999
    'B19001_009E': 'HH_40_45K',  # $40,000 to $44,999
    'B19001_010E': 'HH_45_50K',  # $45,000 to $49,999
    'B19001_011E': 'HH_50_60K',  # $50,000 to $59,999
    'B19001_012E': 'HH_60_75K',  # $60,000 to $74,999
    'B19001_013E': 'HH_75_100K',  # $75,000 to $99,999
    'B19001_014E': 'HH_100_125K',  # $100,000 to $124,999
    'B19001_015E': 'HH_125_150K',  # $125,000 to $149,999
    'B19001_016E': 'HH_150_200K',  # $150,000 to $199,999
    'B19001_017E': 'HH_GT_200K'  # $200,000 or more
}
```

#### ACS_5YR_State

```{python}
# Fetch state boundaries from TIGER/Line shapefiles
gdf_ut_bound = states(
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Filter for Utah only
gdf_ut_bound = gdf_ut_bound[gdf_ut_bound['STATEFP'] == str(STATE_FIPS)]
```

```{python}
# Fetch Income data from ACS 5-year estimates for Utah
df_ut_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"state:{STATE_FIPS}"
    },
    return_geoid=True,
    guess_dtypes=True
)
```

```{python}
# Join ACS data to block group boundaries and transform CRS
gdf_ut_income = gdf_ut_bound[['GEOID', 'STATEFP', 'NAME', 'geometry']].merge(df_ut_income, on = "GEOID").to_crs(PROJECT_CRS)
```

```{python}
# Rename columns
gdf_ut_income = gdf_ut_income.rename(columns=acs_variables)
```

```{python}
# Preview data
gdf_ut_income
```

#### ACS_5YR_County

```{python}
# Fetch county boundaries from TIGER/Line shapefiles
gdf_county_bound = counties(
  state=STATE_FIPS,
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Fetch Income data from ACS 5-year estimates for counties in Utah
df_county_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"county:*",
      "in": f"state:{STATE_FIPS}"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to county boundaries and tranform CRS
gdf_county_income = gdf_county_bound[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(df_county_income, on = "GEOID").to_crs(PROJECT_CRS)

# Rename columns
gdf_county_income = gdf_county_income.rename(columns=acs_variables)

# Preview data
gdf_county_income.head(10)
```

```{python}
# Preview the date in an interactive map
gdf_county_income.explore(column="HH_MED_INC", cmap="YlGnBu", legend=True)
```

```{python}
# Data Validity Check: Coefficient of Variation (CV) for Median Household Income
gdf_county_income['HH_MED_INC_CV'] = (
  gdf_county_income['HH_MED_INC_MOE'] / 1.645
) / gdf_county_income['HH_MED_INC'] * 100

gdf_county_income[['NAMELSAD', 'HH_MED_INC', 'HH_MED_INC_MOE', 'HH_MED_INC_CV']].sort_values(by='HH_MED_INC_CV', ascending=False).head(10)
```

#### ACS_5YR_BlockGroup

```{python}
# Fetch block group boundaries from TIGER/Line shapefiles
gdf_bg_bound = block_groups(
  state=STATE_FIPS,
  year=2023,
  cache=True
).to_crs(PROJECT_CRS)

# Fetch Income data from ACS 5-year estimates for block groups in Utah
df_bg_income = get_census(
  dataset="acs/acs5",
  year=2023,
  variables=list(acs_variables.keys()),
  params={
      # "key": f"{os.getenv('CENSUS_API_KEY')}", # FIXME: This causes error
      "for": f"block group:*",
      "in": f"state:{STATE_FIPS} county:*"
    },
    return_geoid=True,
    guess_dtypes=True
)

# Join ACS data to block group boundaries and transform CRS
gdf_bg_income = gdf_bg_bound[['GEOID', 'COUNTYFP', 'NAMELSAD', 'geometry']].merge(df_bg_income, on = "GEOID").to_crs(PROJECT_CRS)

# Rename columns
gdf_bg_income = gdf_bg_income.rename(columns=acs_variables)

# preview data
gdf_bg_income.head(10)
```

```{python}
#| eval: false

# Preview the date in an interactive map
gdf_bg_income.explore(column="HH_MED_INC", cmap="YlGnBu", legend=True)
```

```{python}
# Data Validity Check
gdf_bg_income[
    gdf_bg_income["HH_MED_INC"].isna() | (gdf_bg_income["HH_MED_INC"] >= 250000)
]
```

#### Chart County Median Income - Scaled by Number of Households

```{python}
# Create plotting dataframe with selected columns
df_plot = gdf_county_income[['NAMELSAD', 'HH_MED_INC', 'HH_TOTAL']].copy()

# Rank counties by median household income (highest = rank 1)
df_plot["Rank"] = df_plot["HH_MED_INC"].rank(
    method="min",       # assign smallest rank for ties
    ascending=False     # highest income gets rank 1
).astype(int)

# Create Region column based on MPO counties
mpo_counties = ['Summit County', 'Wasatch County', 'Davis County', 'Salt Lake County',
                'Utah County', 'Weber County', 'Washington County', 'Cache County', 'Iron County']

df_plot['Region'] = df_plot['NAMELSAD'].apply(lambda x: 'MPO' if x in mpo_counties else 'Non-MPO')

df_plot.sort_values(by="Rank")
```

```{python}
# Define color mapping
color_map = {'MPO': '#843C0C', 'Non-MPO': '#4472C4'}

# Set the style
sns.set_style("whitegrid")

# Create figure
plt.figure(figsize=(10, 6.5))

# Create scatter plot with hue for automatic color separation
sns.scatterplot(
    data=df_plot,
    x='Rank',
    y='HH_MED_INC',
    size='HH_TOTAL',
    sizes=(100, 10000),
    hue='Region',
    palette=color_map,
    alpha=0.3,
    edgecolor=df_plot['Region'].map(color_map),
    linewidth=2.5,
    legend=False
)

# Add smooth trend line
sns.regplot(
    data=df_plot,
    x='Rank',
    y='HH_MED_INC',
    scatter=False,
    order=3,
    ci=None,
    color='red',
    line_kws={'linestyle':':', 'dashes':(1, 1), 'linewidth':2, 'alpha':0.8}
)

# Create list to store text objects
texts = []

# Add labels for each county with color based on Region
for idx, row in df_plot.iterrows():
    county_name = row['NAMELSAD'].replace(' County', '').upper()
    income_label = f"${row['HH_MED_INC']:,.0f}"
    label = f"{county_name}\n{income_label}"
    label_color = color_map[row['Region']]

    text = plt.annotate(
        label,
        xy=(row['Rank'], row['HH_MED_INC']),
        xytext=(row['Rank'], row['HH_MED_INC']),
        textcoords='data',
        ha='center',
        va='center',
        fontsize=7,
        fontweight='bold',
        color=label_color
    )
    texts.append(text)

# Adjust text positions to avoid overlap
adjust_text(
    texts,
    arrowprops=dict(arrowstyle='->', color='gray', lw=0.5, alpha=0.6, shrinkA=10, shrinkB=10),
    # expand_points=(100, 100),  # Increased from (1.5, 1.5)
    # expand_text=(100, 100),    # Added to expand text bounding boxes
    # force_text=(100, 100),     # Increased from (0.5, 0.5)
    # force_points=(100, 100),   # Increased from (0.3, 0.3)
    lim=500                    # Increased iteration limit for better placement
)

# Add horizontal line at median income
state_avg = gdf_ut_income['HH_MED_INC'].values[0]
plt.axhline(y=state_avg, color='#C00000', linestyle='--', linewidth=2, alpha=0.4)

# Add text annotations for state average
plt.text(29.5, state_avg + 1500, f'Utah State Median: ${state_avg:,.0f}', fontsize=10, color='#C00000',
         ha='right', va='bottom', fontweight='bold')

# Set axis limits and ticks
plt.xlim(0, 30)
plt.ylim(0, 150000)
plt.xticks(range(0, 31, 5))

# Format y-axis
plt.gca().yaxis.set_major_locator(ticker.MultipleLocator(20000))
plt.gca().yaxis.set_minor_locator(ticker.MultipleLocator(10000))
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x:,.0f}'))

# Customize grid
plt.grid(True, which='major', axis='y', alpha=0.4, linewidth=0.8, color='gray')
plt.grid(True, which='minor', axis='y', alpha=0.2, linewidth=0.5, color='gray')

# Set labels and title
plt.xlabel('Rank', fontsize=12, fontweight='bold', color='#595959')
plt.ylabel('Median Household Income', fontsize=12, fontweight='bold', color='#595959')
plt.title('Median Income in 2023 Dollars', fontsize=16, fontweight='bold', pad=20, color='#595959')
plt.gca().text(0.5, 1.02, 'Source: 2019-2023 American Community Survey',
               transform=plt.gca().transAxes, ha='center', fontsize=10.5,
               color='#595959', style='italic')

plt.tight_layout()
plt.show()
```

```{python}
# Create scatter plot: County Median Income vs Median of Block Group Median Incomes
# Calculate median of block group median incomes for each county
bg_median_by_county = gdf_bg_income[
    gdf_bg_income['HH_MED_INC'].notna()
].groupby('COUNTYFP')['HH_MED_INC'].median().reset_index()
bg_median_by_county.columns = ['COUNTYFP', 'BG_MedianOfMedians']

# Merge with county data
plot_data = gdf_county_income[['COUNTYFP', 'NAMELSAD', 'HH_MED_INC']].merge(
    bg_median_by_county,
    on='COUNTYFP',
    how='left'
)

# Filter out missing values
plot_data = plot_data[
    (plot_data['HH_MED_INC'].notna()) &
    (plot_data['BG_MedianOfMedians'].notna())
].copy()

# Set style
sns.set_style("whitegrid")

# Create figure
plt.figure(figsize=(10, 8))

# Create scatter plot
sns.scatterplot(
    data=plot_data,
    x='BG_MedianOfMedians',
    y='HH_MED_INC',
    alpha=0.6,
    s=150,
    color='#4472C4',
    edgecolor='#4472C4',
    linewidth=2,
    legend=False
)

# Add regression line
sns.regplot(
    data=plot_data,
    x='BG_MedianOfMedians',
    y='HH_MED_INC',
    scatter=False,
    color='#C00000',
    line_kws={
        'linestyle': '--',
        'linewidth': 2,
        'alpha': 0.7,
        'label': 'Trend line'
    },
    ci=None
)

# Add diagonal reference line (y=x)
min_val = min(plot_data['BG_MedianOfMedians'].min(), plot_data['HH_MED_INC'].min())
max_val = max(plot_data['BG_MedianOfMedians'].max(), plot_data['HH_MED_INC'].max())
plt.plot([min_val, max_val], [min_val, max_val],
         color='gray',
         linestyle=':',
         linewidth=1.5,
         alpha=0.5,
         label='y=x (perfect match)')

# Add county labels
for idx, row in plot_data.iterrows():
    county_name = row['NAMELSAD'].replace(' County', '')
    plt.annotate(
        county_name,
        xy=(row['BG_MedianOfMedians'], row['HH_MED_INC']),
        xytext=(5, 5),
        textcoords='offset points',
        fontsize=8,
        alpha=0.7
    )

# Format axes
plt.gca().xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
plt.gca().yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))

# Labels and title
plt.xlabel('Median of Block Group Median Incomes', fontsize=12, fontweight='bold', color='#595959')
plt.ylabel('County Median Income', fontsize=12, fontweight='bold', color='#595959')
plt.title('County Median Income vs Median of Block Group Median Incomes',
          fontsize=14, fontweight='bold', pad=15, color='#595959')

# Add legend
plt.legend(loc='lower right', fontsize=9)

# Add correlation coefficient and sample size
corr = plot_data['BG_MedianOfMedians'].corr(plot_data['HH_MED_INC'])
plt.text(0.02, 0.98, f'Correlation: {corr:.3f}\nn={len(plot_data)} counties',
         transform=plt.gca().transAxes,
         fontsize=10,
         verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()
```

### BlockSplit_wCOTAZID

## Lookup Tables

### ACS Column ID to Label

```{python}
lookup_hhinc = pd.DataFrame({
  "Income Category": [
    "HH_LT_10K", "HH_10_15K", "HH_15_20K", "HH_20_25K", "HH_25_30K", "HH_30_35K",
    "HH_35_40K", "HH_40_45K", "HH_45_50K", "HH_50_60K", "HH_60_75K",
    "HH_75_100K", "HH_100_125K", "HH_125_150K", "HH_150_200K", "HH_GT_200K"
  ],
  "Lower Limit": [
    0, 10000, 15000, 20000, 25000, 30000,
    35000, 40000, 45000, 50000, 60000,
    75000, 100000, 125000, 150000, 200000
  ],
  "Upper Limit": [
    9999, 14999, 19999, 24999, 29999, 34999,
    39999, 44999, 49999, 59999, 74999,
    99999, 124999, 149999, 199999, np.inf
  ]
})

# Compute midpoint and round it
lookup_hhinc['Midpoint'] = (
  (lookup_hhinc['Lower Limit'] + lookup_hhinc['Upper Limit']) / 2
).round()

# Replace infinite midpoint (last category) with 300000
lookup_hhinc.loc[np.isinf(lookup_hhinc["Upper Limit"]), "Midpoint"] = 300000

lookup_hhinc
```

### BlockGroupID for TAZ Centroid (Use if No HH in TAZ)

```{python}
lookup_hhinc["Income Category"].tolist()
```

## Intermediate Processing



### Process_ACS_BG

- Implement the cumulative sum median calculation
- Handle cases where block groups have no median (use county data)
- Create the processed Block Group dataframe with calculated medians

```{python}
# Get list of income bracket from lookup table
income_bracket_cols = lookup_hhinc["Income Category"].tolist()

# Get midpoint values from lookup table
midpoints = lookup_hhinc['Midpoint'].values

# Prepare working dataset
df_bg_income_process = gdf_bg_income[[
    'GEOID', 'COUNTYFP', 'NAMELSAD', 'HH_TOTAL', 'HH_MED_INC', 'HH_MED_INC_MOE',
    *income_bracket_cols
]].copy()
```

```{python}
# Define the function
def estimate_median(row, income_columns, midpoints):
    """
    Find median income from household income distribution

    row: DataFrame row with household counts by income bracket
    income_columns: List of column names with HH counts (HH_LT_10K, HH_10_15K, etc.)
    midpoints: Array of income bracket midpoints
    """
    total_hh = row['HH_TOTAL']

    # Handle edge cases
    if pd.isna(total_hh) or total_hh == 0:
        return np.nan

    cumsum = 0
    for col, midpoint in zip(income_columns, midpoints):
        # Handle missing values in income brackets
        hh_count = row[col] if pd.notna(row[col]) else 0
        cumsum += hh_count

        if cumsum > total_hh / 2:
            return midpoint

    return 300000  # Default for highest bracket

# Apply the function to each row to calculate estimated median
df_bg_income_process['EstMedInc'] = df_bg_income_process.apply(
    estimate_median,
    axis=1,  # Apply function row-wise
    income_columns=income_bracket_cols,
    midpoints=midpoints
)
```

```{python}
# Assign data quality flags
conditions = [
    # Condition 1: Does the block group have a good ACS reported median?
    (df_bg_income_process['HH_MED_INC'].notna()) &           # Median exists (not missing)
    (df_bg_income_process['HH_MED_INC'] > 0) &               # Median is positive
    (df_bg_income_process['HH_MED_INC'] < 250000),           # Median is reasonable (not extreme)

    # Condition 2: Do we have an estimated median from distribution?
    df_bg_income_process['EstMedInc'].notna()                # Estimated median exists
]

df_bg_income_process['Flag'] = np.select(
    conditions,      # List of conditions to check
    [0, 2],         # Values to assign: 0 if condition 1 True, 2 if condition 2 True
    default=1       # If neither condition is True, assign 1
)
```

```{python}
# Join county median for fallback
df_bg_income_process = df_bg_income_process.merge(
    gdf_county_income[['COUNTYFP', 'HH_MED_INC']].rename(columns={'HH_MED_INC': 'County_MedInc'}),
    on='COUNTYFP',
    how='left'
)
```

```{python}
# Create final median income column based on flag
medinc_conditions = [
    df_bg_income_process['Flag'] == 0,
    df_bg_income_process['Flag'] == 1,
    df_bg_income_process['Flag'] == 2
]

medinc_choices = [
    df_bg_income_process['HH_MED_INC'],
    df_bg_income_process['County_MedInc'],
    df_bg_income_process['EstMedInc']
]

df_bg_income_process['Median Income'] = np.select(medinc_conditions, medinc_choices, default=np.nan)
```

```{python}
# Create final processed dataset
df_bg_income_process = df_bg_income_process[[
    'GEOID', 'COUNTYFP', 'NAMELSAD', 'HH_TOTAL', 'HH_MED_INC',
    'EstMedInc', 'County_MedInc', 'Flag', 'Median Income'
]].copy()

# Display summary
df_bg_income_process.head(10)
```

```{python}
#| eval: false

# Explore the processed block group median income data
gdf_bg_bound[['GEOID', 'geometry']].merge(
    df_bg_income_process,
    on='GEOID',
    how='left'
).explore(
    column="Median Income",
    cmap="YlGnBu",
    legend=True,
    tooltip=["GEOID", "NAMELSAD", "HH_TOTAL", "Median Income", "Flag"]
)
```

### Process_BlockSplit

- Load BlockSplit_wCOTAZID data (you have this file)
- Extract Block Group IDs
- Join with processed ACS data
- Calculate HH-weighted income products

```{python}
# Define variables to download
dec_variables = {
    'H9_001N': 'HH20'           # Total Number of Households
}

# Get census block geometries
gdf_block_bound = blocks(
    state=STATE_FIPS,
    year=2020,
    cache=True
)

# Download decennial census data at block level
df_block_hh = get_census(
    dataset="dec/dhc",
    year=2020,
    variables=list(dec_variables.keys()),
    params={
        # "key": f"{os.getenv('CENSUS_API_KEY')}",
        "for": f"block:*",
        "in": f"state:{STATE_FIPS} county:*"
    },
    return_geoid=True,
    guess_dtypes=True,
)

# Join data to geometry and transform CRS
gdf_block_hh = gdf_block_bound[['GEOID20', 'geometry']].merge(df_block_hh, left_on = "GEOID20", right_on = "GEOID").to_crs(PROJECT_CRS)

# Rename columns
gdf_block_hh = gdf_block_hh.rename(columns=dec_variables)[
    ['GEOID', 'HH20', 'geometry']
]

# Preview data
gdf_block_hh.head(10)
```

```{python}
# Extract Block Group GEOID from Block GEOID (first 12 characters)
gdf_block_hh['GEOID_BG'] = gdf_block_hh['GEOID'].str[:12]

# Add Block Group GEOID to processed block group data
df_bg_income_process['GEOID_BG'] = df_bg_income_process['GEOID']

# Join block group median income and total HH to blocks
gdf_block_income = gdf_block_hh.merge(
    df_bg_income_process[['GEOID_BG', 'HH_TOTAL', 'Median Income']],
    on='GEOID_BG',
    how='left'
)
```

```{python}
# Calculate total 2020 HH per block group
bg_totals_2020 = gdf_block_income.groupby('GEOID_BG')['HH20'].sum().reset_index()
bg_totals_2020.columns = ['GEOID_BG', 'HH20_BG_Total']

# Join block group totals back to blocks
gdf_block_income = gdf_block_income.merge(bg_totals_2020, on='GEOID_BG', how='left')
```

```{python}
# Calculate 2020 proportion for each block
gdf_block_income['Proportion'] = np.where(
    gdf_block_income['HH20_BG_Total'] > 0,
    gdf_block_income['HH20'] / gdf_block_income['HH20_BG_Total'],
    0
)

# Distribute 2023 HH from block group to blocks
gdf_block_income['HH23'] = (
    gdf_block_income['HH_TOTAL'] * gdf_block_income['Proportion']
).round(0).astype('Int64')
```

```{python}
# Create final dataset
gdf_block_final = gdf_block_income[[
    'GEOID',
    'GEOID_BG',
    'HH20',
    'HH23',
    'Median Income',
    'geometry'
]].copy()

gdf_block_final.head(10)
```

### Calc_TAZ_MedInc

- Group by TAZ and calculate weighted averages
- Apply CPI adjustment factors
- Generate both TAZ-level and County-level summaries

```{python}
# Convert blocks to centroids
gdf_block_centroids = gdf_block_final.copy()
gdf_block_centroids['geometry'] = gdf_block_centroids.geometry.centroid
```

```{python}
#| eval: false

# Run this if CO_FIPS and CO_NAME doesn't exist in TAZ file

# Spatial join TAZ with county boundaries to get county information
gdf_TAZ = gpd.sjoin(
    gdf_TAZ,
    gdf_county_bound[['COUNTYFP', 'NAMELSAD', 'geometry']],
    how='left',
    predicate='intersects'
)

# Rename and keep only necessary columns
gdf_TAZ['CO_FIPS'] = gdf_TAZ['COUNTYFP']
gdf_TAZ['CO_NAME'] = gdf_TAZ['NAMELSAD'].str.replace(' County', '').str.upper()

# Drop the extra columns from the join
gdf_TAZ = gdf_TAZ.drop(columns=['index_right', 'COUNTYFP', 'NAMELSAD'])
```

```{python}
# Refine the format for ID columns
gdf_TAZ["CO_TAZID"] = gdf_TAZ["CO_TAZID"].astype(int)
gdf_TAZ["SUBAREAID"] = gdf_TAZ["SUBAREAID"].astype(int)
gdf_TAZ["CO_FIPS"] = gdf_TAZ["CO_FIPS"].astype(int)
```

```{python}
# Spatial join: assign TAZ attributes to block centroids
gdf_block_with_taz = gpd.sjoin(
    gdf_block_centroids,
    gdf_TAZ[['CO_TAZID', 'SUBAREAID', 'CO_FIPS', 'CO_NAME', 'geometry']],
    how='left',
    predicate='within'
)
```

```{python}
# Calculate weighted income (HH * Median Income) for each block
gdf_block_with_taz['HH_MedInc_Product'] = (
    gdf_block_with_taz['HH23'] * gdf_block_with_taz['Median Income']
)
```

```{python}
# Group by TAZ and calculate weighted mean
taz_aggregated = gdf_block_with_taz.groupby('CO_TAZID').agg({
    'SUBAREAID': 'first',
    'CO_FIPS': 'first',
    'CO_NAME': 'first',
    'HH23': 'sum',
    'HH_MedInc_Product': 'sum',
    'Median Income': 'median'  # For fallback if HH23 = 0
}).reset_index()

# Calculate TAZ median income using weighted mean
# If HH > 0, use weighted mean; otherwise use median
taz_aggregated['MedInc'] = np.where(
    taz_aggregated['HH23'].notna() & (taz_aggregated['HH23'] > 0),
    (taz_aggregated['HH_MedInc_Product'] / taz_aggregated['HH23']).round(0),
    taz_aggregated['Median Income'].round(0)
)

taz_aggregated.head(10)
```

```{python}
# Visualize the distribution of TAZ median incomes
gdf_TAZ[['CO_TAZID', 'geometry']].merge(
    taz_aggregated[['CO_TAZID', 'SUBAREAID', 'CO_FIPS', 'CO_NAME', 'MedInc', 'HH23']],
    on='CO_TAZID',
    how='left'
).explore(
    column="MedInc",
    cmap="YlGnBu",
    legend=True,
    tooltip=["CO_TAZID", "MedInc", "HH23"]
)
```

## Export_MedInc

- Format final TAZ output
- Format regional summary

### TAZ Median Income (SE Files)

```{python}
# Create final output
gdf_taz_income = taz_aggregated[[
    'CO_TAZID',
    'SUBAREAID',
    'CO_FIPS',
    'CO_NAME',
    'MedInc'
]].copy()

gdf_taz_income.head(10)
```

```{python}
# Create output directory if it doesn't exist
output_dir = Path("_output")
output_dir.mkdir(parents=True, exist_ok=True)

# Export to CSV
gdf_taz_income.to_csv(
    output_dir / "taz_median_income.csv",
    index=False
)
```

### Regional Median Income (General Parameters)

```{python}
# Create SUBAREAID to Model Space lookup dictionary
subareaid_lookup = {
    0: 'UDOT',
    1: 'Wasatch Front',
    2: 'Cache',
    3: 'Dixie',
    4: 'Summit-Wasatch',
    5: 'Iron'
}
```

```{python}
# Group by SUBAREAID and calculate weighted mean
regional_aggregated = taz_aggregated.groupby('SUBAREAID').agg({
    'HH23': 'sum',
    'HH_MedInc_Product': 'sum',
    'Median Income': 'mean'
}).reset_index()

# Calculate regional median income using weighted mean
regional_aggregated['MedInc'] = np.where(
    regional_aggregated['HH23'] > 0,
    (regional_aggregated['HH_MedInc_Product'] / regional_aggregated['HH23']).round(0),
    regional_aggregated['Median Income'].round(0)
).astype(int)
```

```{python}
# Map SUBAREAID to Model Space names
regional_aggregated['Model Space'] = regional_aggregated['SUBAREAID'].map(subareaid_lookup)

# Create final output
gdf_regional_income = regional_aggregated[[
    'SUBAREAID',
    'Model Space',
    'MedInc'
]].copy()

gdf_regional_income.head(10)
```

```{python}
# Export to CSV
gdf_regional_income.to_csv(
    output_dir / "regional_median_income.csv",
    index=False
)
```

Download the output files: [taz_median_income.csv](./_output/taz_median_income.csv) | [regional_median_income.csv](./_output/regional_median_income.csv)
